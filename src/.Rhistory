'PUNCT' = '\\\\,'
))
regex1 <- sub('ADJ', TARGET, tmp)
regex1 <- str_replace_all(regex1, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
regex2 <- stri_replace_last(tmp, replacement = TARGET, regex = 'ADJ')
regex2 <- str_replace_all(regex2, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
match1 <- unlist(str_extract_all(LEMMA, regex1))
match2 <- unlist(str_extract_all(LEMMA, regex2))
MATCH <- unique(unlist(c(match1, match2)))
if(length(MATCH)==0){return(NULL)}else{
tryCatch(tmp <- lapply(MATCH, function(y){
POS <- spacy_parse(y, pos = T)
if(paste0(POS$pos, collapse = ' ') %in% SYNTAX){
ADJ <- filter(POS, pos == 'ADJ' & !lemma == df$TARGET[INDEX])$lemma
TARGET_mod <- filter(POS, (pos == 'ADJ' & lemma == df$TARGET[INDEX]) | pos == 'ADV')
TARGET_mod <- TARGET_mod$lemma[TARGET_mod$pos == 'ADV' & TARGET_mod$token_id == TARGET_mod$token_id[TARGET_mod$pos == 'ADJ'] - 1]
TARGET_mod <- ifelse(identical(TARGET_mod, character(0)), NA, TARGET_mod)
ADV <- filter(POS, (pos == 'ADJ' & !lemma == df$TARGET[INDEX]) | pos == 'ADV')
ADV <- ADV$lemma[ADV$pos == 'ADV' & ADV$token_id == ADV$token_id[ADV$pos == 'ADJ'] - 1]
ADV <- ifelse(identical(ADV, character(0)), NA, ADV)
first <- filter(POS, pos == 'ADJ')
first <- ifelse(first$token_id[first$lemma == df$TARGET[INDEX]] < first$token_id[!first$lemma == df$TARGET[INDEX]], 1, 0)
dta <- tibble(match = y, ADJ, TARGET_mod, ADV, first)
return(dta)
}
}), warning = function(e) print(INDEX), error = function(e) print(INDEX))
tmp <- do.call(rbind, tmp)
return(tmp)
}
}
for(i in datasets){
print(i)
#i=datasets[1]
load(i)
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
df <- mutate(df, CCONJ = sapply(TARGET, function(x) return(x[x %in% c('and', 'or')][1])))
df <- mutate(df, TARGET = sapply(TARGET, function(x) return(x[x %in% search.terms$word][1])))
df <- mutate(df, comma = grepl('\\,', match))
txtparsed <- spacy_parse(tolower(df$corpus), pos = TRUE)
txtparsed <- split(txtparsed, txtparsed$doc_id, lex.order = F)
txtparsed <- txtparsed[mixedsort(names(txtparsed))]
txtparsed <- pbmclapply(txtparsed, function(x) x$lemma %>% setNames(., x$pos), mc.cores = 4)
system.time(txtparsed_adj <- pbmclapply(1:length(txtparsed), function(g) tryCatch(make_regex(INDEX = g), error = function(e) return(NULL)),  mc.cores = 4))
#df <- rename(df, comma_check = comma, TARGET_check = TARGET, CCONJ_check = CCONJ)
reps <- unlist(lapply(sapply(txtparsed_adj, nrow), function(x) ifelse(is.null(x), 0, x)))
df <- df[rep(1:nrow(df), reps),]
txtparsed_adj <- do.call(rbind, txtparsed_adj)
#txtparsed_adj <- mutate(txtparsed_adj, id=df$id)
#txtparsed_adj <- select(txtparsed_adj, -id)
df <- rename(df, match_first = match)
df <- cbind(df, txtparsed_adj)
df <- as_tibble(df)
#table(df$TARGET)
df <- filter(df, TARGET%in%search.terms$word)
out <- paste0('../../output/02-finalized-corpora/legal/descriptive-', gsub('.*\\/', '', i))
save(df, file=out)
rm(list = c('df', 'txtparsed', 'txtparsed_adj'))
}
library(quanteda)
library(ggplot2)
library(dplyr)
library(pbmcapply)
library(pbapply)
library(lubridate)
library(viridis)
#library(ggwordcloud)
library(car)
#library(nortest)
#library(fastDummies)
library(pbapply)
library(tm)
library(scales)
library(ggrepel)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
kw <- read.table('../input/dict-2.txt', stringsAsFactors = F, sep=',', header = T) %>% rename(TARGET = word)
### load legal data
fileslist <- list.files('../output/02-finalized-corpora/legal', full.names = T, pattern = 'descriptive')
fileslist <- fileslist[!grepl('scotus', fileslist)]
df <- pblapply(fileslist[-length(fileslist)], function(x){
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
return(df)
})
df <- do.call(rbind, df)
### load legal data
fileslist <- list.files('../output/02-finalized-corpora/legal', full.names = T, pattern = 'descriptive')
fileslist
df <- pblapply(fileslist, function(x){
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
return(df)
})
df <- do.call(rbind, df)
# !diagnostics off
library(stringr)
library(stringi)
library(spacyr)
library(gtools)
library(tokenizers)
library(dplyr)
library(reticulate)
library(pbmcapply)
rm(list=ls())
setwd('~/legal_tc/src/legal/')
#spacy_uninstall()
#spacy_install()
#use_condaenv(condaenv = 'spacy_condaenv', conda = "auto", required = FALSE)
spacy_initialize()
#datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T, pattern = 'new')
datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T, pattern = 'descriptive')
datasets <- datasets[!grepl('scotus', datasets)]
#search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- read.table('../../input/descriptive_terms.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- mutate(search.terms, cat = 'descriptive')
search.terms
syntax.regex <- '(ADV\\s)?ADJ\\s(PUNCT\\s)?CCONJ\\s(ADV\\s)?ADJ'
make_regex <- function(INDEX){
TARGET = paste0('\\\\b',df$TARGET[INDEX], '\\\\b')
LEMMA = paste0(txtparsed[[INDEX]], collapse = ' ')
SYNTAX = paste0(names(txtparsed[[INDEX]]), collapse = ' ')
SYNTAX <- unlist(str_extract(SYNTAX, syntax.regex))
SYNTAX <- unique(SYNTAX)
tmp <- str_replace_all(SYNTAX, c(
'ADV' = '\\\\w+',
'CCONJ' = 'and',
'PUNCT' = '\\\\,'
))
regex1 <- sub('ADJ', TARGET, tmp)
regex1 <- str_replace_all(regex1, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
regex2 <- stri_replace_last(tmp, replacement = TARGET, regex = 'ADJ')
regex2 <- str_replace_all(regex2, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
match1 <- unlist(str_extract_all(LEMMA, regex1))
match2 <- unlist(str_extract_all(LEMMA, regex2))
MATCH <- unique(unlist(c(match1, match2)))
if(length(MATCH)==0){return(NULL)}else{
tryCatch(tmp <- lapply(MATCH, function(y){
POS <- spacy_parse(y, pos = T)
if(paste0(POS$pos, collapse = ' ') %in% SYNTAX){
ADJ <- filter(POS, pos == 'ADJ' & !lemma == df$TARGET[INDEX])$lemma
TARGET_mod <- filter(POS, (pos == 'ADJ' & lemma == df$TARGET[INDEX]) | pos == 'ADV')
TARGET_mod <- TARGET_mod$lemma[TARGET_mod$pos == 'ADV' & TARGET_mod$token_id == TARGET_mod$token_id[TARGET_mod$pos == 'ADJ'] - 1]
TARGET_mod <- ifelse(identical(TARGET_mod, character(0)), NA, TARGET_mod)
ADV <- filter(POS, (pos == 'ADJ' & !lemma == df$TARGET[INDEX]) | pos == 'ADV')
ADV <- ADV$lemma[ADV$pos == 'ADV' & ADV$token_id == ADV$token_id[ADV$pos == 'ADJ'] - 1]
ADV <- ifelse(identical(ADV, character(0)), NA, ADV)
first <- filter(POS, pos == 'ADJ')
first <- ifelse(first$token_id[first$lemma == df$TARGET[INDEX]] < first$token_id[!first$lemma == df$TARGET[INDEX]], 1, 0)
dta <- tibble(match = y, ADJ, TARGET_mod, ADV, first)
return(dta)
}
}), warning = function(e) print(INDEX), error = function(e) print(INDEX))
tmp <- do.call(rbind, tmp)
return(tmp)
}
}
i=datasets[1]
datasets
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
#i=datasets[1]
load(i)
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
df <- mutate(df, CCONJ = sapply(TARGET, function(x) return(x[x %in% c('and', 'or')][1])))
df <- mutate(df, TARGET = sapply(TARGET, function(x) return(x[x %in% search.terms$word][1])))
df <- mutate(df, comma = grepl('\\,', match))
txtparsed <- spacy_parse(tolower(df$corpus), pos = TRUE)
txtparsed <- split(txtparsed, txtparsed$doc_id, lex.order = F)
txtparsed <- txtparsed[mixedsort(names(txtparsed))]
txtparsed <- pbmclapply(txtparsed, function(x) x$lemma %>% setNames(., x$pos), mc.cores = 4)
txtparsed
system.time(txtparsed_adj <- pbmclapply(1:length(txtparsed), function(g) tryCatch(make_regex(INDEX = g), error = function(e) return(NULL)),  mc.cores = 4))
txtparsed_adj
#df <- rename(df, comma_check = comma, TARGET_check = TARGET, CCONJ_check = CCONJ)
reps <- unlist(lapply(sapply(txtparsed_adj, nrow), function(x) ifelse(is.null(x), 0, x)))
df <- df[rep(1:nrow(df), reps),]
txtparsed_adj <- do.call(rbind, txtparsed_adj)
#txtparsed_adj <- mutate(txtparsed_adj, id=df$id)
#txtparsed_adj <- select(txtparsed_adj, -id)
df <- rename(df, match_first = match)
df <- cbind(df, txtparsed_adj)
df <- as_tibble(df)
#table(df$TARGET)
df <- filter(df, TARGET%in%search.terms$word)
out <- paste0('../../output/02-finalized-corpora/legal/descriptive-', gsub('.*\\/', '', i))
save(df, file=out)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
kw <- read.table('../input/dict-2.txt', stringsAsFactors = F, sep=',', header = T) %>% rename(TARGET = word)
### load legal data
fileslist <- list.files('../output/02-finalized-corpora/legal', full.names = T, pattern = 'descriptive')
fileslist <- fileslist[!grepl('scotus', fileslist)]
fileslist
x=fileslist[1]
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
df <- pblapply(fileslist, function(x){
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
return(df)
})
df <- do.call(rbind, df)
df
# !diagnostics off
library(stringr)
library(stringi)
library(spacyr)
library(gtools)
library(tokenizers)
library(dplyr)
library(reticulate)
library(pbmcapply)
rm(list=ls())
setwd('~/legal_tc/src/legal/')
#spacy_uninstall()
#spacy_install()
#use_condaenv(condaenv = 'spacy_condaenv', conda = "auto", required = FALSE)
spacy_initialize()
#datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T, pattern = 'new')
datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T, pattern = 'descriptive')
datasets <- datasets[!grepl('scotus', datasets)]
#search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- read.table('../../input/descriptive_terms.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- mutate(search.terms, cat = 'descriptive')
syntax.regex <- '(ADV\\s)?ADJ\\s(PUNCT\\s)?CCONJ\\s(ADV\\s)?ADJ'
make_regex <- function(INDEX){
TARGET = paste0('\\\\b',df$TARGET[INDEX], '\\\\b')
LEMMA = paste0(txtparsed[[INDEX]], collapse = ' ')
SYNTAX = paste0(names(txtparsed[[INDEX]]), collapse = ' ')
SYNTAX <- unlist(str_extract(SYNTAX, syntax.regex))
SYNTAX <- unique(SYNTAX)
tmp <- str_replace_all(SYNTAX, c(
'ADV' = '\\\\w+',
'CCONJ' = 'and',
'PUNCT' = '\\\\,'
))
regex1 <- sub('ADJ', TARGET, tmp)
regex1 <- str_replace_all(regex1, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
regex2 <- stri_replace_last(tmp, replacement = TARGET, regex = 'ADJ')
regex2 <- str_replace_all(regex2, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
match1 <- unlist(str_extract_all(LEMMA, regex1))
match2 <- unlist(str_extract_all(LEMMA, regex2))
MATCH <- unique(unlist(c(match1, match2)))
if(length(MATCH)==0){return(NULL)}else{
tryCatch(tmp <- lapply(MATCH, function(y){
POS <- spacy_parse(y, pos = T)
if(paste0(POS$pos, collapse = ' ') %in% SYNTAX){
ADJ <- filter(POS, pos == 'ADJ' & !lemma == df$TARGET[INDEX])$lemma
TARGET_mod <- filter(POS, (pos == 'ADJ' & lemma == df$TARGET[INDEX]) | pos == 'ADV')
TARGET_mod <- TARGET_mod$lemma[TARGET_mod$pos == 'ADV' & TARGET_mod$token_id == TARGET_mod$token_id[TARGET_mod$pos == 'ADJ'] - 1]
TARGET_mod <- ifelse(identical(TARGET_mod, character(0)), NA, TARGET_mod)
ADV <- filter(POS, (pos == 'ADJ' & !lemma == df$TARGET[INDEX]) | pos == 'ADV')
ADV <- ADV$lemma[ADV$pos == 'ADV' & ADV$token_id == ADV$token_id[ADV$pos == 'ADJ'] - 1]
ADV <- ifelse(identical(ADV, character(0)), NA, ADV)
first <- filter(POS, pos == 'ADJ')
first <- ifelse(first$token_id[first$lemma == df$TARGET[INDEX]] < first$token_id[!first$lemma == df$TARGET[INDEX]], 1, 0)
dta <- tibble(match = y, ADJ, TARGET_mod, ADV, first)
return(dta)
}
}), warning = function(e) print(INDEX), error = function(e) print(INDEX))
tmp <- do.call(rbind, tmp)
return(tmp)
}
}
datasets
for(i in datasets[-1]){
print(i)
#i=datasets[1]
load(i)
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
df <- mutate(df, CCONJ = sapply(TARGET, function(x) return(x[x %in% c('and', 'or')][1])))
df <- mutate(df, TARGET = sapply(TARGET, function(x) return(x[x %in% search.terms$word][1])))
df <- mutate(df, comma = grepl('\\,', match))
txtparsed <- spacy_parse(tolower(df$corpus), pos = TRUE)
txtparsed <- split(txtparsed, txtparsed$doc_id, lex.order = F)
txtparsed <- txtparsed[mixedsort(names(txtparsed))]
txtparsed <- pbmclapply(txtparsed, function(x) x$lemma %>% setNames(., x$pos), mc.cores = 4)
system.time(txtparsed_adj <- pbmclapply(1:length(txtparsed), function(g) tryCatch(make_regex(INDEX = g), error = function(e) return(NULL)),  mc.cores = 4))
#df <- rename(df, comma_check = comma, TARGET_check = TARGET, CCONJ_check = CCONJ)
reps <- unlist(lapply(sapply(txtparsed_adj, nrow), function(x) ifelse(is.null(x), 0, x)))
df <- df[rep(1:nrow(df), reps),]
txtparsed_adj <- do.call(rbind, txtparsed_adj)
#txtparsed_adj <- mutate(txtparsed_adj, id=df$id)
#txtparsed_adj <- select(txtparsed_adj, -id)
df <- rename(df, match_first = match)
df <- cbind(df, txtparsed_adj)
df <- as_tibble(df)
#table(df$TARGET)
df <- filter(df, TARGET%in%search.terms$word)
out <- paste0('../../output/02-finalized-corpora/legal/descriptive-', gsub('.*\\/', '', i))
save(df, file=out)
rm(list = c('df', 'txtparsed', 'txtparsed_adj'))
}
library(quanteda)
library(ggplot2)
library(dplyr)
library(pbmcapply)
library(pbapply)
library(lubridate)
library(viridis)
#library(ggwordcloud)
library(car)
#library(nortest)
#library(fastDummies)
library(pbapply)
library(tm)
library(scales)
library(ggrepel)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
#kw <- read.table('../input/dict-2.txt', stringsAsFactors = F, sep=',', header = T) %>% rename(TARGET = word)
kw <- read.table('../input/descriptive_terms.txt', stringsAsFactors = F, sep=',', header = T) %>% rename(TARGET = word)
### load legal data
fileslist <- list.files('../output/02-finalized-corpora/legal', full.names = T, pattern = 'descriptive')
fileslist <- fileslist[!grepl('scotus', fileslist)]
df <- pblapply(fileslist, function(x){
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
return(df)
})
df <- do.call(rbind, df)
# combine data
df <- as_tibble(df)
# filter the right years, the right target adjectives, and change context variable to a dummy
df <- mutate(df, year = as.numeric(year))
df <- filter(df, year > 1979 & year < 2021)
df <- filter(df, TARGET %in% kw$TARGET)
df <- mutate(df, context = ifelse(context == 'reddit', context, 'court'))
# load the sentiment dictionary
load('../res/sentiWords-db.RDS')
# annotate data
annot <- tokens(df$ADJ)
annot <- tokens_lookup(annot, dictionary = sentiWords$num)
#rm(sentiWords)
annot <- sapply(annot, function(x) mean(as.numeric(unlist(x)),  na.rm = T))
df$sentiWords <- annot
rm(annot)
dfx <- df
prblm <- c('too', 'not', 'less')
nrow(filter(df, TARGET_mod%in%prblm))/nrow(df)
nrow(filter(df, TARGET_mod%in%c('so', 'very', 'really')))/nrow(df)
dfx <- dfx %>% filter(!TARGET_mod%in%prblm)
nrow(filter(df, ADV%in%prblm))/nrow(df)
dfx <- dfx %>% filter(!ADV%in%prblm)
prblm <- c('most', 'many', 'more', 'non', 'other', 'last', 'overall', 'much', 'idk', 'holy', 'such')
nrow(filter(df, ADJ%in%prblm))/nrow(df)
dfx <- dfx %>% filter(!ADJ%in%prblm)
df %>% filter(!is.na(CCONJ)) %>% filter(CCONJ%in%c('and', 'but')) %>% nrow/nrow(df)
dfx <- dfx %>% filter(!is.na(CCONJ)|CCONJ%in%c('and', 'but'))
dfx <- filter(dfx, !is.na(sentiWords))
# get aggregates
dfx <- left_join(dfx, kw)
#rm(sentiWords)
#means <- dfx %>% group_by(TARGET, cat, CCONJ, context) %>% summarise(sentiWords = mean(sentiWords, na.rm = T))
#dfx <- dfx %>% filter(!(is.na(sentiWords)|is.na(cat)|is.na(CCONJ)))
dfx <- dfx %>% filter(!(is.na(sentiWords)|is.na(CCONJ)))
annot <- tokens_lookup(tokens(unique(dfx$TARGET)), dictionary = sentiWords$dichot)
annot <- tibble(TARGET = unique(dfx$TARGET), TARGET_pol = sapply(annot, function(x) x[1]))
annot <- annot %>% mutate(TARGET_pol = ifelse(TARGET %in% c('arbitrary', 'illegal'), 'negative', TARGET_pol))
dfx <- left_join(dfx, annot)
dfx
### compute diversity measures
div_mes <- dfx %>%
group_by(TARGET) %>%
summarise(ADJ_full = paste0(ADJ, collapse = ' '))
div_mes
res_div <- div_mes %>%
quanteda::tokens(.) %>%
textstat_lexdiv(measure = c("TTR", "CTTR", "K"))
res_div <- div_mes %>%
quanteda::tokens(ADJ_full) %>%
textstat_lexdiv(measure = c("TTR", "CTTR", "K"))
res_div <- quanteda::tokens(div_mes$aADJ_full) %>%
textstat_lexdiv(measure = c("TTR", "CTTR", "K"))
res_div <- quanteda::tokens(div_mes$ADJ_full) %>%
textstat_lexdiv(measure = c("TTR", "CTTR", "K"))
head(res_div)
res_div <- div_mes$ADJ_full
div_mes$ADJ_full
res_div <- quanteda::tokens(div_mes$ADJ_full)
textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
div_mes
res_div$TARGET <- div_mes$TARGET
res_div
### compute diversity measures
div_mes <- dfx %>%
group_by(TARGET) %>%
summarise(ADJ_full = paste0(ADJ, collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div$TARGET <- div_mes$TARGET
### compute diversity measures
div_mes <- dfx %>%
group_by(TARGET) %>%
summarise(ADJ_full = paste0(ADJ, collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- cbind(res_div, div_mes[, c('TARGET', 'n')], )
div_mes
div_mes[, c('TARGET', 'n')]
res_div <- cbind(res_div, div_mes[, c('TARGET', 'n')])
res_div
plot(TTR, n, data = res_div)
plot(res_div$TTR, res_div$n)
res_div %>% arrange(K)
res_div %>% arrange(CTTR)
res_div %>% arrange(desc(CTTR))
res_div %>% arrange(desc(K))
res_div %>% arrange(desc(CTTR)) %>% slice_head(prop = 0.5)
res_div %>% arrange(desc(CTTR)) %>% slice_head(prop = 0.5) %>% filter(n >= 500)
write.csv(res_div, file = '../output/03-results/tables/diversity-analysis-descriptive.txt')
write.csv(res_div, file = '../output/03-results/tables/diversity-analysis-descriptive.txt', quote = F, row.names = F)
fileslist <- list.files('/Volumes/INTENSO/legal_tc/output/01-inductive-approach-only-ADJ', full.names = T)
df <- lapply(fileslist, function(x){
load(x)
return(df)
})
df <- do.call(rbind, df)
aggr <- df %>% group_by(ADJ1, ADJ2) %>%
summarise(n = sum(n), sentiWords = unique(sentiWords)) %>%
mutate(perc = n/sum(n),
total = sum(n))
nope <- c(
'one', 'two', 'three', 'five', 'four',
'fourth', 'second', 'fifth', 'first',
'black', 'other', 'eight', 'non', 'eighth',
'full'
)
fileslist <- list.files('/Volumes/INTENSO/legal_tc/output/01-inductive-approach-only-ADJ', full.names = T)
df <- lapply(fileslist, function(x){
load(x)
return(df)
})
df
df <- do.call(rbind, df)
df
quanteda::tokens(df$ADJ1, remove_punct = TRUE)
### diversity analysis
df$ADJ1 <- quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separartors = TRUE)
### diversity analysis
df$ADJ1 <- quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE)
fileslist <- list.files('/Volumes/INTENSO/legal_tc/output/01-inductive-approach-only-ADJ', full.names = T)
df <- lapply(fileslist, function(x){
load(x)
return(df)
})
df <- do.call(rbind, df)
### diversity analysis
df$ADJ1 <- quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE)
fileslist <- list.files('/Volumes/INTENSO/legal_tc/output/01-inductive-approach-only-ADJ', full.names = T)
df <- lapply(fileslist, function(x){
load(x)
return(df)
})
df <- do.call(rbind, df)
df$ADJ1
quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE)
quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE) %>%
tokens_select(., pattern = stopwords("en"), selection = "remove")
### diversity analysis
df$ADJ1 <- quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE) %>% unlist
quanteda::tokens(df$ADJ1, remove_punct = TRUE, remove_separators = TRUE)
### diversity analysis
df %>%
group_by(ADJ1) %>%
summarise(ADJ_full = paste0(rep(ADJ2, n), collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- cbind(res_div, div_mes[, c('TARGET', 'n')])
### diversity analysis
div_mes <- df %>%
group_by(ADJ1) %>%
summarise(ADJ_full = paste0(rep(ADJ2, n), collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- cbind(res_div, div_mes[, c('AD1', 'n')])
res_div <- cbind(res_div, div_mes[, c('ADJ1', 'n')])
res_div
res_div %>% arrange(desc(n))
res_div %>% filter(n >= 50) %>% write.csv(., file = '../output/03-results/tables/diversity-analysis-inductive.txt')
res_div %>% filter(n >= 50) %>% write.csv(., file = '../output/03-results/tables/diversity-analysis-inductive.txt', quote = F, row.names = F)
res_div %>% filter(n >= 50) %>% arrange(desc(n)) %>% write.csv(., file = '../output/03-results/tables/diversity-analysis-inductive.txt', quote = F, row.names = F)
