means <- left_join(means, annot)
means <- means %>% arrange(cat, TARGET_pol, desc(avg_conj_sentiment))
print(means, n=200)
write.csv(means, file = '../../output/03-results/tables/LC_avg_conj_sentiment.csv', quote = F, row.names = F)
### compute diversity measures to decide which descriptive adjectives to keep
div_mes <- df %>%
group_by(TARGET) %>%
summarise(ADJ_full = paste0(ADJ, collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- cbind(res_div, div_mes[, c('TARGET', 'n')])
res_div <- res_div %>% left_join(., kw) %>% left_join(., annot)
plot(res_div$TTR, res_div$n)
res_div
plot(res_div$TTR, res_div$n)
# write out results
res_div <- res_div %>% arrange(cat, TARGET_pol, desc(n), K)
write.csv(res_div, file = '../../output/03-results/tables/LC_diversity-analysis.csv', quote = F, row.names = F)
### get final list of target adjectives
f_res_div <- res_div %>% filter(!((cat == 'legal' & TARGET_pol == 'negative') | (TARGET_pol == c('neutral') | (cat == 'epistemic') | n <= 200))) %>% group_by(cat, TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.75) %>% print(., n=200)
# negative legal concepts only have 5 entries rather than 6, so that we have to slice differently
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'legal' & TARGET_pol == 'negative' & n >= 200) %>% arrange(K))
res_div
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% arrange(desc(n))
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K, desc(n))
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.4)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.6)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.5)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.75) %>% arrange(desc(n)) %>% slice_head(prop = 0.25)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n))
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.25)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.4)
res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.3)
### get final list of target adjectives
f_res_div <- res_div %>% filter(!((cat == 'legal' & TARGET_pol == 'negative') | (TARGET_pol == c('neutral') | (cat == 'epistemic') | n <= 200))) %>% group_by(cat, TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.75) %>% print(., n=200)
# negative legal concepts only have 5 entries rather than 6, so that we have to slice differently
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'legal' & TARGET_pol == 'negative' & n >= 200) %>% arrange(K))
# descriptive concepts have more than 6 entries, so that we have to slice differently
# three only: f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive') %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.25))
# neg
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.5))
# pos
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.3))
# same for epistemic concepts
f_res_div <- f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'epistemic') %>% group_by(TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.5))
# write out results
f_res_div <- f_res_div %>% arrange(cat, TARGET_pol, K)
print(f_res_div, n=200)
### load legal data
fileslist <- list.files('../../output/02-finalized-corpora/legal', full.names = T, pattern = 'descriptive|new')
fileslist <- fileslist[!grepl('scotus', fileslist)]
fileslist
df <- pblapply(fileslist, function(x){
load(x)
df <- mutate(df, context = gsub('\\.RDS|.*\\/', '', x))
return(df)
})
# combine data
df <- do.call(rbind, df)
df <- as_tibble(df)
# filter for years after 1979, the right target adjectives, and change context variable to a dummy
df <- mutate(df, year = as.numeric(year))
df <- filter(df, year > 1979 & year < 2021)
df <- filter(df, TARGET %in% kw$TARGET)
df <- mutate(df, context = ifelse(context == 'reddit', context, 'court'))
### sentiment annotation
# load the sentiment dictionary
load('../../res/sentiWords-db.RDS')
# annotate conjoined adjectives
annot <- tokens(df$ADJ)
annot <- tokens_lookup(annot, dictionary = sentiWords$num)
annot <- sapply(annot, function(x) mean(as.numeric(unlist(x)),  na.rm = T))
df$sentiWords <- annot
rm(annot)
# annotate target adjectives
df <- left_join(df, kw)
annot <- tokens_lookup(tokens(unique(df$TARGET)), dictionary = sentiWords$dichot)
annot <- tibble(TARGET = unique(df$TARGET), TARGET_pol = sapply(annot, function(x) x[1]))
# manually fix NAs
annot <- annot %>% mutate(
TARGET_pol_new = case_when(
TARGET == 'complex' ~ 'negative',
TARGET %in% c('illegal', 'arbitrary') ~ 'negative',
TARGET %in% c('appropriate') ~ 'positive',
TRUE ~ TARGET_pol
),
TARGET_pol = case_when(
TARGET %in% c('arbitrary', 'illegal') ~ 'negative',
TARGET %in% c('appropriate') ~ 'positive',
TARGET %in% kw2$TARGET ~ 'neutral',
TRUE ~ TARGET_pol)
)
df <- left_join(df, annot)
### filter out problematic metadata
dfx <- df
## get rid of entries containing a negative modifier-adverb
prblm <- c('too', 'not', 'less')
dfx <- dfx %>% filter(!(TARGET_mod%in%prblm | ADV%in%prblm))
# data loss
nrow(filter(df, TARGET_mod%in%prblm | ADV%in%prblm))/nrow(df)
## get rid of enries containing messy conjuncts
prblm <- c('most', 'many', 'more', 'non', 'other', 'last', 'overall', 'much', 'idk', 'holy', 'such')
dfx <- dfx %>% filter(!ADJ%in%prblm)
# data loss
nrow(filter(df, ADJ%in%prblm))/nrow(df)
# make sure that only AND-CCONJ are retained
table(dfx$CCONJ, useNA = 'always')
# get rid of entries without sentiment values for conjoined adjective
dfx <- filter(dfx, !is.na(sentiWords))
# overwrite old corpus
df <- dfx
rm(dfx)
### write out average conjoined sentiment for target adjectives
means <- df %>% group_by(TARGET, cat) %>% summarise(avg_conj_sentiment = mean(sentiWords, na.rm = T))
means <- left_join(means, annot)
means <- means %>% arrange(cat, TARGET_pol, desc(avg_conj_sentiment))
print(means, n=200)
write.csv(means, file = '../../output/03-results/tables/LC_avg_conj_sentiment.csv', quote = F, row.names = F)
### compute diversity measures to decide which descriptive adjectives to keep
div_mes <- df %>%
group_by(TARGET) %>%
summarise(ADJ_full = paste0(ADJ, collapse = ' '),
n = n())
res_div <- quanteda::tokens(div_mes$ADJ_full)
res_div <- textstat_lexdiv(res_div, measure = c("TTR", "CTTR", "K"))
res_div <- cbind(res_div, div_mes[, c('TARGET', 'n')])
res_div <- res_div %>% left_join(., kw) %>% left_join(., annot)
plot(res_div$TTR, res_div$n)
# write out results
res_div <- res_div %>% arrange(cat, TARGET_pol, desc(n), K)
write.csv(res_div, file = '../../output/03-results/tables/LC_diversity-analysis.csv', quote = F, row.names = F)
### get final list of target adjectives
f_res_div <- res_div %>% filter(!((cat == 'legal' & TARGET_pol == 'negative') | (TARGET_pol == c('neutral') | (cat == 'epistemic') | n <= 200))) %>% group_by(cat, TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.75) %>% print(., n=200)
# negative legal concepts only have 5 entries rather than 6, so that we have to slice differently
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'legal' & TARGET_pol == 'negative' & n >= 200) %>% arrange(K))
# descriptive concepts have more than 6 entries, so that we have to slice differently
# three only: f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive') %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.25))
# neg
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.5))
# pos
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.3))
# same for epistemic concepts
f_res_div <- f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'epistemic') %>% group_by(TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.5))
# write out results
f_res_div <- f_res_div %>% arrange(cat, TARGET_pol, K)
print(f_res_div, n=200)
# write out results
f_res_div <- f_res_div %>% arrange(cat, TARGET_pol, K)
print(f_res_div, n=200)
### get final list of target adjectives
f_res_div <- res_div %>% filter(!((cat == 'legal' & TARGET_pol == 'negative') | (TARGET_pol == c('neutral') | (cat == 'epistemic') | n <= 200))) %>% group_by(cat, TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.75) %>% print(., n=200)
# negative legal concepts only have 5 entries rather than 6, so that we have to slice differently
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'legal' & TARGET_pol == 'negative' & n >= 200) %>% arrange(K))
# descriptive concepts have more than 6 entries, so that we have to slice differently
# three only: f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive') %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.25))
# neg
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'negative') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(desc(n), K) %>% slice_head(prop = 0.5))
# pos
f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'descriptive' & TARGET_pol_new == 'positive') %>% na.omit %>% group_by(TARGET_pol_new) %>% arrange(K) %>% slice_head(prop = 0.5) %>% arrange(desc(n)) %>% slice_head(prop = 0.3))
# same for epistemic concepts
f_res_div <- f_res_div <- rbind(f_res_div, res_div %>% filter(cat == 'epistemic') %>% group_by(TARGET_pol) %>% arrange(K) %>% slice_head(prop = 0.5))
# write out results
f_res_div <- f_res_div %>% arrange(cat, TARGET_pol, K)
print(f_res_div, n=200)
write.csv(f_res_div, file = '../../output/03-results/tables/LC_final_list.csv', quote = F, row.names = F)
### filter the adjectives that are not needed
df <- filter(df, TARGET%in%f_res_div$TARGET)
### clean corpus text for vector space analysis
vec <- df$corpus
df$corpus_clean <- tolower(vec) %>%
gsub("<.*?>", ' ', .) %>%
removePunctuation() %>%
removeNumbers() %>%
removeWords(., stopwords('en')) %>%
stripWhitespace()
## save corpus
save(df, file = '../../output/02-finalized-corpora/legal/LC_consolidated.RDS', compress = 'gzip')
library(quanteda)
library(ggplot2)
library(dplyr)
library(purrr)
library(xtable)
library(envalysis)
library(gridExtra)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
# function to compute quantiles per group in dplyr
p <- c(0.25, 0.5, 0.75)
p_names <- map_chr(p, ~paste0(.x*100, "%"))
p_funs <- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %>%
set_names(nm = p_names)
# function to compute diversity measures
k <- c('TTR', 'CTTR', 'K')
k_funs <- map(k, ~partial(textstat_lexdiv, measure = .x)) %>%
set_names(nm = k)
# load data
load('../output/02-finalized-corpora/baseline/reddit/BC_consolidated.RDS')
bc <- df
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
lc <- df
rm(df)
### write out adjective list with aggregates
df <- rbind(bc, select(lc, -id, -year, -court)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
### compute aggregates for each corpus
## LC
# quantiles
df_quants <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
df_avg <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
df_div <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
df_div <- cbind(df_div, textstat_lexdiv(tokens(df_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
df_div <- select(df_div, -conjuncts, -document)
# combine
df_summary <- left_join(df_quants, df_avg) %>% left_join(., df_div) %>% arrange(cat, TARGET_pol, TARGET)
# write out results
write.csv(df_summary, file = '../output/03-results/tables/COMBINED_summary_stats.csv', quote = F, row.names = F)
df_summary <- xtable(df_summary)
print(df_summary, include.rownames =F , file = '../output/03-results/tables/COMBINED_summary_stats.tex')
### write out adjective list with aggregates
df <- rbind(bc, select(lc, -id, -year, -court, -TARGET_pol)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
lc
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
lc <- df
rm(df)
### write out adjective list with aggregates
df <- rbind(bc, select(lc, -id, -year, -court, -TARGET_pol)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC', descriptive = 'Descriptive'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
select(lc, -id, -year, -court, -TARGET_pol)
### write out adjective list with aggregates
df <- rbind(mutate(bc, TARGET_pol_new = TARGET_pol), select(lc, -id, -year, -court)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC', descriptive = 'Descriptive'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
### write out adjective list with aggregates
# quantiles
df_quants <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
df_avg <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
df_div <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
df_div <- cbind(df_div, textstat_lexdiv(tokens(df_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
df_div <- select(df_div, -conjuncts, -document)
# combine
df_summary <- left_join(df_quants, df_avg) %>% left_join(., df_div) %>% arrange(cat, TARGET_pol, TARGET)
df_summary
### write out adjective list with aggregates
# quantiles
df_quants <- df %>%
group_by(cat, TARGET, TARGET_pol_new) %>%
summarize_at(vars(sentiWords), p_funs)
# average
df_avg <- df %>%
group_by(cat, TARGET, TARGET_pol_new) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
df_div <- df %>%
group_by(cat, TARGET, TARGET_pol_new) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
df_div <- cbind(df_div, textstat_lexdiv(tokens(df_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
df_div <- select(df_div, -conjuncts, -document)
# combine
df_summary <- left_join(df_quants, df_avg) %>% left_join(., df_div) %>% arrange(cat, TARGET_pol_new, TARGET)
df_summary
# write out results
write.csv(df_summary, file = '../output/03-results/tables/COMBINED_summary_stats.csv', quote = F, row.names = F)
df_summary <- xtable(df_summary)
print(df_summary, include.rownames =F , file = '../output/03-results/tables/COMBINED_summary_stats.tex')
## LC
# quantiles
lc_quants <- lc %>%
group_by(cat, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
lc_avg <- lc %>%
group_by(cat, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
lc_div <- lc %>%
group_by(cat, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
## LC
# quantiles
lc_quants <- lc %>%
group_by(cat, TARGET_pol_new) %>%
summarize_at(vars(sentiWords), p_funs)
# average
lc_avg <- lc %>%
group_by(cat, TARGET_pol_new) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
lc_div <- lc %>%
group_by(cat, TARGET_pol_new) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
lc_div <- cbind(lc_div, textstat_lexdiv(tokens(lc_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
lc_div <- select(lc_div, -conjuncts, -document)
# combine
lc_summary <- left_join(lc_quants, lc_avg) %>% left_join(., lc_div)
lc_summary
# write out results
write.csv(lc_summary, file = '../output/03-results/tables/LC_summary_stats.csv', quote = F, row.names = F)
lc_summary <- xtable(lc_summary)
print(lc_summary, include.rownames =F , file = '../output/03-results/tables/LC_summary_stats.tex')
## BC
# quantiles
bc_quants <- bc %>%
group_by(cat, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
bc_avg <- bc %>%
group_by(cat, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
bc_div <- bc %>%
group_by(cat, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
bc_div <- cbind(bc_div, textstat_lexdiv(tokens(bc_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
bc_div <- select(bc_div, -conjuncts, -document)
# combine
bc_summary <- left_join(bc_quants, bc_avg) %>% left_join(., bc_div)
# write out results
write.csv(bc_summary, file = '../output/03-results/tables/BC_summary_stats.csv', quote = F, row.names = F)
bc_summary <- xtable(bc_summary)
print(bc_summary, include.rownames =F , file = '../output/03-results/tables/BC_summary_stats.tex')
# !diagnostics off
library(quanteda)
library(ggplot2)
library(dplyr)
library(emmeans)
library(xtable)
library(factoextra)
library(Ckmeans.1d.dp)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
# load data
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
# combine to full corpus
df <- df %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC', descriptive = 'Desc.'))
# differences between concept classes
m1 <- lm(abs(sentiWords) ~ cat, data = df)
emm1 <- emmeans(m1, specs = pairwise ~ cat)
res1 <- xtable(emm1$contrasts)
print(res1, include.rownames = F)
# K-Means Cluster Analysis
aggr <- df %>%
#mutate(group = paste0(TARGET, '_', TARGET_pol)) %>%
#group_by(group) %>%
group_by(cat, TARGET) %>%
summarise(means = mean(abs(sentiWords), na.rm =T),
lex = paste0(ADJ, collapse = ' '))
# differences between concept classes
m2 <- lm(sentiWords ~ cat*TARGET_pol_new, data = df)
emm2 <- emmeans(m2, specs = pairwise ~ cat)
emm2 <- emmeans(m2, specs = pairwise ~ cat, by ='TARGET_pol_new')
res2 <- xtable(emm2$contrasts)
res2
emm2
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
# function to compute quantiles per group in dplyr
p <- c(0.25, 0.5, 0.75)
p_names <- map_chr(p, ~paste0(.x*100, "%"))
p_funs <- map(p, ~partial(quantile, probs = .x, na.rm = TRUE)) %>%
set_names(nm = p_names)
# function to compute diversity measures
k <- c('TTR', 'CTTR', 'K')
k_funs <- map(k, ~partial(textstat_lexdiv, measure = .x)) %>%
set_names(nm = k)
# load data
load('../output/02-finalized-corpora/baseline/reddit/BC_consolidated.RDS')
bc <- df
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
lc <- df
rm(df)
### write out adjective list with aggregates
df <- rbind(mutate(bc, TARGET_pol_new = TARGET_pol), select(lc, -id, -year, -court)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC', descriptive = 'Descriptive'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
### write out adjective list with aggregates
# quantiles
df_quants <- df %>%
group_by(cat, TARGET, TARGET_pol_new) %>%
summarize_at(vars(sentiWords), p_funs)
### write out adjective list with aggregates
# quantiles
df_quants <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
df_avg <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
df_div <- df %>%
group_by(cat, TARGET, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
df_div <- cbind(df_div, textstat_lexdiv(tokens(df_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
df_div <- select(df_div, -conjuncts, -document)
# combine
df_summary <- left_join(df_quants, df_avg) %>% left_join(., df_div) %>% arrange(cat, TARGET_pol, TARGET)
df_summary
# write out results
write.csv(df_summary, file = '../output/03-results/tables/COMBINED_summary_stats.csv', quote = F, row.names = F)
df_summary <- xtable(df_summary)
print(df_summary, include.rownames =F , file = '../output/03-results/tables/COMBINED_summary_stats.tex')
## LC
# quantiles
lc_quants <- lc %>%
group_by(cat, TARGET_pol) %>%
summarize_at(vars(sentiWords), p_funs)
# average
lc_avg <- lc %>%
group_by(cat, TARGET_pol) %>%
summarise(mean = mean(sentiWords, na.rm = T))
# diversity
lc_div <- lc %>%
group_by(cat, TARGET_pol) %>%
summarize(conjuncts = paste0(ADJ, collapse = ' '),
n = n())
lc_div <- cbind(lc_div, textstat_lexdiv(tokens(lc_div$conjuncts), measure = c('TTR', 'CTTR', 'K')))
lc_div <- select(lc_div, -conjuncts, -document)
# combine
lc_summary <- left_join(lc_quants, lc_avg) %>% left_join(., lc_div)
# write out results
write.csv(lc_summary, file = '../output/03-results/tables/LC_summary_stats.csv', quote = F, row.names = F)
lc_summary <- xtable(lc_summary)
lc_summary
print(lc_summary, include.rownames =F , file = '../output/03-results/tables/LC_summary_stats.tex')
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# function for linebreaks in plots
abbrv <- function(x, width = 200) lapply(strwrap(x, width, simplify = FALSE), paste, collapse="\n")
# load data
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
# combine to full corpus
df <- df %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC', descriptive = 'Desc.'))
# differences between concept classes
m1 <- lm(abs(sentiWords) ~ cat, data = df)
emm1 <- emmeans(m1, specs = pairwise ~ cat)
res1 <- xtable(emm1$contrasts)
print(res1, include.rownames = F)
m1a <- anova(abs(sentiWords) ~ cat, data = df)
m1a <- aov(abs(sentiWords) ~ cat, data = df)
m1a
emmeans(m1a, specs = pairwise ~ cat)
# load data
load('../output/02-finalized-corpora/baseline/reddit/BC_consolidated.RDS')
bc <- df
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
lc <- df
rm(df)
# combine to full corpus
df <- rbind(bc, select(lc, -id, -year, -court)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
# filter out descriptive target adjectives for study 1
df <- filter(df, !TARGET_pol == 'neutral')
# load data
load('../output/02-finalized-corpora/baseline/reddit/BC_consolidated.RDS')
bc <- df %>% mutate(TARGET_pol_new = TARGET_pol)
load('../output/02-finalized-corpora/legal/LC_consolidated.RDS')
lc <- df
rm(df)
# combine to full corpus
df <- rbind(bc, select(lc, -id, -year, -court)) %>%
mutate(cat = dplyr::recode(cat, epistemic = 'Epistemic', legal = 'Legal', tc = 'TC'),
context = dplyr::recode(context, court = 'LC', reddit = 'BC'))
# filter out descriptive target adjectives for study 1
df <- filter(df, !TARGET_pol == 'neutral')
### H1a & H1b
m2 <- lm(sentiWords ~ context * TARGET_pol, data = df)
anova(m2)
m2a <- aov(sentiWords ~ context * TARGET_pol, data = df)
emm2 <- emmeans(m2, specs = pairwise ~ context, at = list(.group = c("BC", "LC")), by = 'TARGET_pol')
emm2
emmeans(m2a, specs = pairwise ~ context, at = list(.group = c("BC", "LC")), by = 'TARGET_pol')
