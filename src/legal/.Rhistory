gsub('rude\\sand', '', p)
tokens_lookup(tokens(trims(gsub('rude\\sand', '', p)), dictionary = data_dictionary_LSD2015)
tokens_lookup(tokens(trims(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015)
tokens_lookup(tokens(trims(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015)
tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015)
table(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015))
as.character(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015))
table(as.character(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015)))
as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015))
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p))), dictionary = data_dictionary_LSD2015))
lp
lp
lp[lp=='positive']
names(lp[lp=='positive'])
lp[lp=='positive'] %>% names %>% gsub('doc', '', .) %>% as.numeric
lp[lp=='positive'] %>% names %>% gsub('doc', '', .) %>% as.numeric %>% p[.]
p
lp[lp=='positive'] %>% names %>% p[.]
library(udpipe)
library(quanteda)
df <- read.csv('~/Dropbox/thesis/scripts/test.csv', header = T, stringsAsFactors = F)
udmodel <- udpipe_load_model(file = '~/udpipe/english-ud-2.0-170801.udpipe')
dta <- df$txt %>% trimws %>% gsub('^.*(?=(rude\\sand))', '', ., perl = T)
dta <- udpipe_annotate(udmodel, dta)
dta <- as.data.frame(dta) %>% as_tibble
dta
dta <- split(dta, dta$doc_id)
snippetize <- function(x){
if(all(x$upos[1:3]==c('ADJ', 'CCONJ', 'ADJ'))){
return(paste0(x$token[1:3], collapse = ' '))
}
if(FALSE){
}
#else if(any(x$upos[3:6]%in%c('ADJ', 'NOUN'))){
#  return(x$token[1:grep('ADJ|NOUN', x$upos)[2]])
#}
else{
return(NULL)
}
}
p <- lapply(dta, snippetize)
p
df <- df %>% mutate(docid=paste0('doc', id))
df
p <- lapply(dta, snippetize)
p <- tibble(snippet=unlist(p), docid=names(p))
names(p)
unlist(p)
is.null(p)
sapply(is.null, p)
sapply(p, is.null)
p <- lapply(dta, snippetize)
p <- tibble(snippet=unlist(p), docid=names(p)[!sapply(p, is.null)])
p
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p$snippet))), dictionary = data_dictionary_LSD2015))
lp
rbind(p, lp)
lp
lp[sapply(lp, is.null)] <- 'no sentiment available'
lp
sapply(lp, is.null)
p
p <- lapply(dta, snippetize)
p
sapply(lp, function(x) x==character(0))
character(0)
is.character(character(0))
lp
lp
sapply(lp, function(x) !x%in%c('positive, negative'))
sapply(lp, function(x) !unlist(x)%in%c('positive, negative'))
sapply(lp, function(x) !unlist(x)%in%c('positive', 'negative'))
sapply(lp, function(x) x==character(0)
)
sapply(lp, function(x) identical(x, character(0)))
lp[sapply(lp, function(x) identical(x, character(0)))] <- 'no sentiment available'
lp
lp <- unlist(lp)
rbind(p, lp)
cbind(p, lp)
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p$snippet))), dictionary = data_dictionary_LSD2015))
p <- lapply(dta, snippetize)
p <- tibble(snippet=unlist(p), docid=names(p)[!sapply(p, is.null)])
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p$snippet))), dictionary = data_dictionary_LSD2015))
lp[sapply(lp, function(x) identical(x, character(0)))] <- 'no sentiment available'
lp
nrow(p)
length(lp)
lp <- unlist(lp)
lp
cbind(p, lp)
p <- cbind(p, lp)
left_join(df, p, by='docid')
left_join(p, df, by='docid')
fin <- left_join(p, df, by='docid')
write.csv(fin,'~/Dropbox/thesis/scripts/test_annot.csv', row.names = F, ccol.names = T)
write.csv(fin,'~/Dropbox/thesis/scripts/test_annot.csv', row.names = F, col.names = T)
write.csv(fin,'~/Dropbox/thesis/scripts/test_annot.csv', row.names = F)
df <- read.csv('~/Dropbox/thesis/scripts/test.csv', header = T, stringsAsFactors = F)
df <- df %>% mutate(docid=paste0('doc', id))
udmodel <- udpipe_load_model(file = '~/udpipe/english-ud-2.0-170801.udpipe')
dta <- df$txt %>% trimws %>% gsub('^.*(?=(rude\\sand))', '', ., perl = T)
dta <- udpipe_annotate(udmodel, dta)
dta <- as.data.frame(dta) %>% as_tibble
dta <- split(dta, dta$doc_id)
snippetize <- function(x){
if(all(x$upos[1:3]==c('ADJ', 'CCONJ', 'ADJ'))){
return(paste0(x$token[1:3], collapse = ' '))
}
if(FALSE){
}
#else if(any(x$upos[3:6]%in%c('ADJ', 'NOUN'))){
#  return(x$token[1:grep('ADJ|NOUN', x$upos)[2]])
#}
else{
return(NULL)
}
}
p <- lapply(dta, snippetize)
p <- tibble(snippet=unlist(p), docid=names(p)[!sapply(p, is.null)])
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p$snippet))), dictionary = data_dictionary_LSD2015))
lp[sapply(lp, function(x) identical(x, character(0)))] <- 'no sentiment available'
lp
lp <- unlist(lp)
p <- cbind(p, lp)
p
p <- lapply(dta, snippetize)
p <- tibble(snippet=unlist(p), docid=names(p)[!sapply(p, is.null)])
lp <- as.list(tokens_lookup(tokens(trimws(gsub('rude\\sand', '', p$snippet))), dictionary = data_dictionary_LSD2015))
lp[sapply(lp, function(x) identical(x, character(0)))] <- 'no sentiment available'
lp <- unlist(lp)
p <- cbind(p, sentiment=lp)
p
fin <- left_join(p, df, by='docid')
write.csv(fin,'~/Dropbox/thesis/scripts/test_annot.csv', row.names = F)
fin %>% filter(sentiment=='positive')
fin
table(p$sentiment)
table(p$sentiment)/nrow(p)
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
str(FEEL_fr)
table(FEEL_fr$y, useNA = 'always')
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
library(dplyr)
table(FEEL_fr$y, useNA = 'always')
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
dict <- lapply(dict, function(x) select(x, -sentiment))
dict
dict <- dictionary(dict)
###########################################################
# Setup
library(tidyverse)
library(quanteda)
dict <- dictionary(dict)
tokens_lookup('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même',
dictionary = dict, valuetype = "glob")
tokens_lookup(tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même'),
dictionary = dict, valuetype = "glob")
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
test
str(dict)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict
dict <- split(dict, dict$sentiment)
dict <- lapply(dict, function(x) select(x, -sentiment))
dict
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict
dict <- dictionary(dict)
dict <- split(dict, dict$sentiment)
dict <- lapply(dict, function(x) select(x, -sentiment))
dict <- dictionary(dict)
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
str(dict)
l <- tokens_lookup(test,
dictionary = dict)
list(positive = c('Révolution', 'comme', 'apéro', 'grève', 'monsieur', 'omelette'),
negative = c('Dire', 'et'))
p <- list(positive = c('Révolution', 'comme', 'apéro', 'grève', 'monsieur', 'omelette'),
negative = c('Dire', 'et'))
dict2 <- dictionary(p)
dict2
l <- tokens_lookup(test,
dictionary = dict2)
l
p
dict2 <- dictionary(p)
dict2
l <- tokens_lookup(test,
dictionary = dict2)
l
str(FEEL_fr)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
sum(lengths(dict))
sum(sapply(dict, nrow))
dict <- lapply(dict, function(x) select(x, -sentiment))
sum(sapply(dict, nrow))
head(dict$negative, 500)
head(dict$negative, 500) %>%  as.vector()
head(dict$negative, 500) %>% unlist
library(pbmcapply)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
set.seed(123)
dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict
dict <- lapply(dict, function(x) select(x, -sentiment))
dict <- dictionary(dict)
system.time(
l <- tokens_lookup(test,
dictionary = dict)
)
system.time(
l <- pbmclapply(unlist(test), function(x) tokens_lookup(tokens(x),
dictionary = dict), mc.cores =5)
)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+', x$word), ])
dict
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
dict
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+', x$word), ])
dict
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
dict$negative
dict <- lapply(dict, function(x) select(x, -sentiment))
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+|\\|', x$word), ])
dict <- lapply(dict, function(x) select(x, -sentiment))
head(dict$negative, 500) %>% unlist
sum(sapply(dict, nrow))
library(udpipe)
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
str(FEEL_fr)
dict <- as_tibble(FEEL_fr) %>% rename(polarity = y, term = x)
dict
l <- txt_sentiment(test, polarity_terms = dict)
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
udpipe(test, "french-spoken", trace = 10)
test <- 'Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même'
udpipe(test, "french-spoken", trace = 10)
test <- udpipe(test, "french-spoken", trace = 10)
l <- txt_sentiment(test, polarity_terms = dict)
l
l
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/valence-raw/valShifters.rda"))
polarity_terms <- rename(FEEL_fr, term = x, polarity = y)
polarity_negators <- subset(valShifters$valence_fr, t == 1)$x
polarity_amplifiers <- subset(valShifters$valence_fr, t == 2)$x
polarity_deamplifiers <- subset(valShifters$valence_fr, t == 3)$x
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(test, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments
polarity_negators
polarity_amplifiers
load("~/Downloads/NR_SR_Jan_Okt_2019.RData")
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
txt <- romandie_tweets2$Text
head(txt)
txt <- txt[sample(1:length(txt), 200)]
txt
romandie_tweets2 <- udpipe(txt, "french-spoken", trace = 10)
romandie_tweets2
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(txt, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(romandie_tweets2, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments
# Prepping the dict
romandie_udpipe <- udpipe(txt, "french-spoken", trace = 10)
sentiments
# graph
library(magrittr)
library(ggraph)
library(igraph)
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
sentiments
# sentiment
sentiments_txt <- txt_sentiment(romandie_udpipe, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments <- sentiments_txt$data
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
reasons <- filter(reasons, dep_rel %in% "amod")
word_cooccurences <- reasons %>%
group_by(lemma, lemma_parent) %>%
summarise(cooc = n()) %>%
arrange(-cooc)
vertices <- bind_rows(
data_frame(key = unique(reasons$lemma)) %>% mutate(in_dictionary = if_else(key %in% polarity_terms$term, "in_dictionary", "linked-to")),
data_frame(key = unique(setdiff(reasons$lemma_parent, reasons$lemma))) %>% mutate(in_dictionary = "linked-to"))
# graph
library(magrittr)
library(ggraph)
library(igraph)
cooc <- head(word_cooccurences, 20)
set.seed(123)
cooc %>%
graph_from_data_frame(vertices = filter(vertices, key %in% c(cooc$lemma, cooc$lemma_parent))) %>%
ggraph(layout = "fr") +
geom_edge_link0(aes(edge_alpha = cooc, edge_width = cooc)) +
geom_node_point(aes(colour = in_dictionary), size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
ggtitle("Which words are linked to the negative terms") +
theme_void()
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 200)]
head(romandie_txt)
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_tweets2$Text
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_tweets2
romandie_tweets2$Text
head(romandie_tweets2)
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
romandie_tweets2
romandie_tweets2
romandie_tweets2$Text
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 200)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_udpipe <- udpipe(romandie_txt, "french-spoken", trace = 10)  # only using a small sample first to figure out all the settings and not waste time
# sentiment
sentiments_txt <- txt_sentiment(romandie_udpipe, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments <- sentiments_txt$data
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
reasons <- filter(reasons, dep_rel %in% "amod")
word_cooccurences <- reasons %>%
group_by(lemma, lemma_parent) %>%
summarise(cooc = n()) %>%
arrange(-cooc)
vertices <- bind_rows(
data_frame(key = unique(reasons$lemma)) %>% mutate(in_dictionary = if_else(key %in% polarity_terms$term, "in_dictionary", "linked-to")),
data_frame(key = unique(setdiff(reasons$lemma_parent, reasons$lemma))) %>% mutate(in_dictionary = "linked-to"))
cooc <- head(word_cooccurences, 20)
set.seed(123)
cooc %>%
graph_from_data_frame(vertices = filter(vertices, key %in% c(cooc$lemma, cooc$lemma_parent))) %>%
ggraph(layout = "fr") +
geom_edge_link0(aes(edge_alpha = cooc, edge_width = cooc)) +
geom_node_point(aes(colour = in_dictionary), size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
ggtitle("Which words are linked to the negative terms") +
theme_void()
install.packages('qualtRics')
library(qualtRics)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
filepath <- '/Users/lucienbaumgartner/Dropbox/thesis/embed.R/input/TC+Embeddings+Valence+Control_April+28,+2020_09.13.csv'
library(reticulate)
os <- import("os")
os$listdir(".")
Sys.which("python")
#spacy_install()
use_condaenv("myenv")
spacyr::spacy_initialize(condaenv = "myenv")
#spacy_install()
use_condaenv("myenv", required = T)
#spacy_install()
use_virtualenv("myenv")
spacyr::spacy_initialize(virtualenv =  = "myenv")
spacyr::spacy_initialize(virtualenv  = "myenv")
spacyr::spacy_initialize()
#spacy_install()
use_python("/Users/lucienbaumgartner/.conda/envs/spacy_condaenv/bin/python")
spacyr::spacy_initialize()
spacyr::spacy_initialize(python_executable = '/Users/lucienbaumgartner/.conda/envs/spacy_condaenv/bin/python')
library(sparklyr)
sc <- spark_connect(master = "local", version = "3.0")
options(sparklyr.java9 = TRUE)
sc <- spark_connect(master = "local", version = "3.0")
spark_disconnect(sc)
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
library(pbmcapply)
library(dplyr)
rm(list=ls())
setwd('~/legal_tc/src/legal/')
bytes <- 1500*1024^2
options(future.globals.maxSize = bytes)
getOption('future.globals.maxSize')
#datasets <- list.files('../../output/00-bulk-data/legal', full.names = T)
datasets <- list.files('/Volumes/INTENSO/legal_tc/output/00-bulk-data/legal', full.names = T)
datasets <- datasets[!grepl('scotus', datasets)]
#search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- read.table('../../input/descriptive_terms.txt', header = T, stringsAsFactors = F, sep=',')
search.terms
#search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms <- read.table('../../input/descriptive_terms.txt', header = T, stringsAsFactors = F, sep=',')
search.term <- mutate(search.terms, cat = 'descriptive')
search.term
for(i in datasets){
#i = datasets[1]
print(i)
load(i)
.lookup <- paste0(paste0('(\\w+(\\,)?\\s\\band\\b\\s\\b', search.terms$word, '\\b', ')|(\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b\\s\\w+)', collapse = '|')
df <- mutate(df, year = as.numeric(year))
df <- filter(df, year > 1979)
set.seed(12345)
df <- df %>% ungroup %>% sample_frac(.6)
if(object.size(df) > 1000*1024^2){
print('splitting corpus')
vec <- list()
vec[[1]] <- df$txt[1:(round(length(df$txt)/2, 0))]
vec[[2]] <- df$txt[(round(length(df$txt)/2, 0)+1):length(df$txt)]
for(m in 1:2){
vec[[m]] <- pbmclapply(vec[[m]], function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
corpus <- append(vec[[1]], vec[[2]])
length(corpus) == nrow(df)
}else{
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
#df
# save data
out <- paste0('../../output/01-reduced-corpora/legal/descriptive-', gsub('.*\\/', '', i))
save(df, file = out)
rm(df)
rm(reg_matches)
}
