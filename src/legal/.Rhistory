dfx <- mutate(dfx, cat = ifelse(cat == 'epistmic', 'epistemic', cat))
#.lvls <- dfx %>% arra
overall_means <- dfx %>%
group_by(cat, TARGET_pol) %>%
summarise(avg = mean(sentiWords, na.rm = T))
#p <- ggplot(dfx, aes(y=sentiWords, x=context, fill=cat)) +
p <- ggplot(dfx, aes(y=sentiWords, x=TARGET)) +
geom_hline(aes(yintercept=0), lty='dashed') +
geom_boxplot(aes( fill=cat), outlier.shape = NA) +
#geom_point(data = means, aes(y=sentiWords, colour=cat)) +
geom_point(data = means, aes(y=sentiWords)) +
geom_point(data = means, aes(y=sentiWords), shape=1) +
geom_hline(data = overall_means, aes(yintercept = avg, colour = cat)) +
facet_grid(~ TARGET_pol, scales = 'free_x', drop = T) +
#scale_color_manual(values = rev(cols)) +
#scale_fill_manual(values = rev(cols)) +
guides(color = FALSE) +
labs(
title = 'Sentiment Distribution of Adjective Conjunctions',
subtitle = abbrv(paste0('The data consists of ',
format(nrow(dfx), big.mark = "'"),
' court opinions from the US Supreme Court (SCOTUS) and the US Court of Appeals (1st to 11th Circuit). The boxes represent the quartiles, the whiskers +/-1.5*IQR, the horizontal line the median, and the dots the means.'
), width = 130),
y = 'sentiWords Score of conjoined adjectives\nfor lemma#pos:#a (adjectives)',
caption =
abbrv(
paste0('NUMBER OF NON-UNIQUE ADJ:   ',
paste0(
names(table(dfx$TARGET[!is.na(dfx$sentiWords)])),
': ',
format(as.character(table(dfx$TARGET[!is.na(dfx$sentiWords)]), big.mark = "'")),
collapse = '; '
)
)
, width = 170)
) +
theme(
plot.title = element_text(face= 'bold'),
axis.text.x = element_text(angle = 45, hjust = 1)
)
p
nope <- c(
'adequate', 'cruel', 'constitutional',
'arbitrary', 'fair', 'factual',
'unlawful', 'lawful'
)
dfx <- dfx %>% filter(!TARGET %in% nope)
means <- dfx %>% group_by(TARGET, CCONJ) %>% summarise(sentiWords = mean(sentiWords, na.rm = T))
means <- left_join(means, annot)
dfx %>% group_by(TARGET, context) %>% summarise(n = n())
means <- means %>%
arrange(TARGET_pol, desc(sentiWords)) %>%
mutate(TARGET = factor(TARGET, levels = TARGET))
dfx <- dfx %>% mutate(TARGET = factor(TARGET, levels=means$TARGET))
dfx <- mutate(dfx, cat = ifelse(cat == 'epistmic', 'epistemic', cat))
#.lvls <- dfx %>% arra
overall_means <- dfx %>%
group_by(cat, TARGET_pol) %>%
summarise(avg = mean(sentiWords, na.rm = T))
#p <- ggplot(dfx, aes(y=sentiWords, x=context, fill=cat)) +
p <- ggplot(dfx, aes(y=sentiWords, x=TARGET)) +
geom_hline(aes(yintercept=0), lty='dashed') +
geom_boxplot(aes( fill=cat), outlier.shape = NA) +
#geom_point(data = means, aes(y=sentiWords, colour=cat)) +
geom_point(data = means, aes(y=sentiWords)) +
geom_point(data = means, aes(y=sentiWords), shape=1) +
geom_hline(data = overall_means, aes(yintercept = avg, colour = cat)) +
facet_grid(~ TARGET_pol, scales = 'free_x', drop = T) +
#scale_color_manual(values = rev(cols)) +
#scale_fill_manual(values = rev(cols)) +
guides(color = FALSE) +
labs(
title = 'Sentiment Distribution of Adjective Conjunctions',
subtitle = abbrv(paste0('The data consists of ',
format(nrow(dfx), big.mark = "'"),
' court opinions from the US Supreme Court (SCOTUS) and the US Court of Appeals (1st to 11th Circuit). The boxes represent the quartiles, the whiskers +/-1.5*IQR, the horizontal line the median, and the dots the means.'
), width = 130),
y = 'sentiWords Score of conjoined adjectives\nfor lemma#pos:#a (adjectives)',
caption =
abbrv(
paste0('NUMBER OF NON-UNIQUE ADJ:   ',
paste0(
names(table(dfx$TARGET[!is.na(dfx$sentiWords)])),
': ',
format(as.character(table(dfx$TARGET[!is.na(dfx$sentiWords)]), big.mark = "'")),
collapse = '; '
)
)
, width = 170)
) +
theme(
plot.title = element_text(face= 'bold'),
axis.text.x = element_text(angle = 45, hjust = 1)
)
p
dfx <- df
prblm <- c('too', 'not', 'less')
nrow(filter(df, TARGET_mod%in%prblm))/nrow(df)
nrow(filter(df, TARGET_mod%in%c('so', 'very', 'really')))/nrow(df)
dfx <- dfx %>% filter(!TARGET_mod%in%prblm)
nrow(filter(df, ADV%in%prblm))/nrow(df)
dfx <- dfx %>% filter(!ADV%in%prblm)
prblm <- c('most', 'many', 'more', 'non', 'other', 'last', 'overall', 'much', 'idk', 'holy', 'such')
nrow(filter(df, ADJ%in%prblm))/nrow(df)
dfx <- dfx %>% filter(!ADJ%in%prblm)
df %>% filter(!is.na(CCONJ)) %>% filter(CCONJ%in%c('and', 'but')) %>% nrow/nrow(df)
dfx <- dfx %>% filter(!is.na(CCONJ)|CCONJ%in%c('and', 'but'))
dfx <- filter(dfx, !is.na(sentiWords))
# get aggregates
dfx <- left_join(dfx, kw)
#rm(sentiWords)
#means <- dfx %>% group_by(TARGET, cat, CCONJ, context) %>% summarise(sentiWords = mean(sentiWords, na.rm = T))
#dfx <- dfx %>% filter(!(is.na(sentiWords)|is.na(cat)|is.na(CCONJ)))
dfx <- dfx %>% filter(!(is.na(sentiWords)|is.na(CCONJ)))
annot <- tokens_lookup(tokens(unique(dfx$TARGET)), dictionary = sentiWords$dichot)
annot <- tibble(TARGET = unique(dfx$TARGET), TARGET_pol = sapply(annot, function(x) x[1]))
annot <- annot %>% mutate(TARGET_pol = ifelse(TARGET %in% c('arbitrary', 'illegal'), 'negative', TARGET_pol))
dfx <- left_join(dfx, annot)
#nope <- c(
#  'adequate', 'cruel', 'constitutional',
#  'arbitrary', 'fair', 'factual',
#  'unlawful', 'lawful'
#)
#dfx <- dfx %>% filter(!TARGET %in% nope)
means <- dfx %>% group_by(TARGET, CCONJ) %>% summarise(sentiWords = mean(sentiWords, na.rm = T))
means <- left_join(means, annot)
dfx %>% group_by(TARGET, context) %>% summarise(n = n())
means <- means %>%
arrange(TARGET_pol, desc(sentiWords)) %>%
mutate(TARGET = factor(TARGET, levels = TARGET))
dfx <- dfx %>% mutate(TARGET = factor(TARGET, levels=means$TARGET))
dfx <- mutate(dfx, cat = ifelse(cat == 'epistmic', 'epistemic', cat))
#.lvls <- dfx %>% arra
overall_means <- dfx %>%
group_by(cat, TARGET_pol) %>%
summarise(avg = mean(sentiWords, na.rm = T))
#p <- ggplot(dfx, aes(y=sentiWords, x=context, fill=cat)) +
p <- ggplot(dfx, aes(y=sentiWords, x=TARGET)) +
geom_hline(aes(yintercept=0), lty='dashed') +
geom_boxplot(aes( fill=cat), outlier.shape = NA) +
#geom_point(data = means, aes(y=sentiWords, colour=cat)) +
geom_point(data = means, aes(y=sentiWords)) +
geom_point(data = means, aes(y=sentiWords), shape=1) +
geom_hline(data = overall_means, aes(yintercept = avg, colour = cat)) +
facet_grid(~ TARGET_pol, scales = 'free_x', drop = T) +
#scale_color_manual(values = rev(cols)) +
#scale_fill_manual(values = rev(cols)) +
guides(color = FALSE) +
labs(
title = 'Sentiment Distribution of Adjective Conjunctions',
subtitle = abbrv(paste0('The data consists of ',
format(nrow(dfx), big.mark = "'"),
' court opinions from the US Supreme Court (SCOTUS) and the US Court of Appeals (1st to 11th Circuit). The boxes represent the quartiles, the whiskers +/-1.5*IQR, the horizontal line the median, and the dots the means.'
), width = 130),
y = 'sentiWords Score of conjoined adjectives\nfor lemma#pos:#a (adjectives)',
caption =
abbrv(
paste0('NUMBER OF NON-UNIQUE ADJ:   ',
paste0(
names(table(dfx$TARGET[!is.na(dfx$sentiWords)])),
': ',
format(as.character(table(dfx$TARGET[!is.na(dfx$sentiWords)]), big.mark = "'")),
collapse = '; '
)
)
, width = 170)
) +
theme(
plot.title = element_text(face= 'bold'),
axis.text.x = element_text(angle = 45, hjust = 1)
)
p
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
library(pbmcapply)
library(dplyr)
rm(list=ls())
setwd('~/legal_tc/src/legal/')
bytes <- 1500*1024^2
options(future.globals.maxSize = bytes)
getOption('future.globals.maxSize')
#datasets <- list.files('../../output/00-bulk-data/legal', full.names = T)
datasets <- list.files('/Volumes/INTENSO/legal_tc/output/00-bulk-data/legal', full.names = T)
datasets <- datasets[!grepl('scotus', datasets)]
datasets
search.terms <- read.table('../../input/dict-2.txt', header = T, stringsAsFactors = F, sep=',')
search.terms
search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms
.lookup <- paste0(paste0('(\\w+(\\,)?\\s\\band\\b\\s\\b', search.terms$word, '\\b', ')|(\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b\\s\\w+)', collapse = '|')
.lookup
for(i in datasets){
#i = datasets[1]
print(i)
load(i)
.lookup <- paste0(paste0('(\\w+(\\,)?\\s\\band\\b\\s\\b', search.terms$word, '\\b', ')|(\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b\\s\\w+)', collapse = '|')
df <- mutate(df, year = as.numeric(year))
df <- filter(df, year > 1979)
if(object.size(df) > 1000*1024^2){
print('splitting corpus')
vec <- list()
vec[[1]] <- df$txt[1:(round(length(df$txt)/2, 0))]
vec[[2]] <- df$txt[(round(length(df$txt)/2, 0)+1):length(df$txt)]
for(m in 1:2){
vec[[m]] <- pbmclapply(vec[[m]], function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
corpus <- append(vec[[1]], vec[[2]])
length(corpus) == nrow(df)
}else{
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
#df
# save data
out <- paste0('../../output/01-reduced-corpora/legal/new-', gsub('.*\\/', '', i))
save(df, file = out)
rm(df)
rm(reg_matches)
}
for(i in datasets){
#i = datasets[1]
print(i)
load(i)
.lookup <- paste0(paste0('(\\w+(\\,)?\\s\\band\\b\\s\\b', search.terms$word, '\\b', ')|(\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b\\s\\w+)', collapse = '|')
df <- mutate(df, year = as.numeric(year))
df <- filter(df, year > 1979)
set.seed(12345)
df <- df %>% ungroup %>% sample_frac(.6)
if(object.size(df) > 1000*1024^2){
print('splitting corpus')
vec <- list()
vec[[1]] <- df$txt[1:(round(length(df$txt)/2, 0))]
vec[[2]] <- df$txt[(round(length(df$txt)/2, 0)+1):length(df$txt)]
for(m in 1:2){
vec[[m]] <- pbmclapply(vec[[m]], function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
corpus <- append(vec[[1]], vec[[2]])
length(corpus) == nrow(df)
}else{
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
}
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
#df
# save data
out <- paste0('../../output/01-reduced-corpora/legal/new-', gsub('.*\\/', '', i))
save(df, file = out)
rm(df)
rm(reg_matches)
}
library(dplyr)
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
library(pbmcapply)
library(stringr)
rm(list=ls())
setwd('~/legal_tc/src/baseline/')
datasets <- list.files('../../output/01-reduced-corpora/baseline/reddit', full.names = T)
datasets
library(dplyr)
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
library(pbmcapply)
rm(list=ls())
setwd('~/legal_tc/src/baseline/')
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T, pattern = 'new\\-')
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T, pattern = 'new-')
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T, pattern = 'new')
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T)
datasets
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T, pattern = '\\%22')
datasets
search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
search.terms
for(i in datasets){
#i = datasets[1]
load(i)
df <- tibble(txt=dta)
.lookup <- paste0(paste0('(\\w+(\\,)?\\s\\band\\b\\s\\b', search.terms$word, '\\b', ')|(\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b\\s\\w+)', collapse = '|')
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
# save data
out <- paste0('../../output/01-reduced-corpora/baseline/reddit/new-', gsub('.*\\/', '', i))
save(df, file = out, compress = 'gzip')
}
library(dplyr)
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
library(pbmcapply)
library(stringr)
rm(list=ls())
setwd('~/legal_tc/src/baseline/')
datasets <- list.files('../../output/01-reduced-corpora/baseline/reddit', full.names = T, pattern = 'new')
datasets
search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
syntax.regex <- '(ADV\\s)?ADJ\\s(PUNCT\\s)?CCONJ\\s(ADV\\s)?ADJ'
make_regex <- function(INDEX){
TARGET = paste0('\\\\b',df$TARGET[INDEX], '\\\\b')
LEMMA = paste0(txtparsed[[INDEX]], collapse = ' ')
SYNTAX = paste0(names(txtparsed[[INDEX]]), collapse = ' ')
SYNTAX <- unlist(str_extract(SYNTAX, syntax.regex))
SYNTAX <- unique(SYNTAX)
tmp <- str_replace_all(SYNTAX, c(
'ADV' = '\\\\w+',
'CCONJ' = 'and',
'PUNCT' = '\\\\,'
))
regex1 <- sub('ADJ', TARGET, tmp)
regex1 <- str_replace_all(regex1, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
regex2 <- stri_replace_last(tmp, replacement = TARGET, regex = 'ADJ')
regex2 <- str_replace_all(regex2, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
match1 <- unlist(str_extract_all(LEMMA, regex1))
match2 <- unlist(str_extract_all(LEMMA, regex2))
MATCH <- unlist(c(match1, match2))
tryCatch(tmp <- lapply(MATCH, function(y){
POS <- spacy_parse(y, pos = T)
if(paste0(POS$pos, collapse = ' ') %in% SYNTAX){
ADJ <- filter(POS, pos == 'ADJ' & !lemma == df$TARGET[INDEX])$lemma
TARGET_mod <- filter(POS, (pos == 'ADJ' & lemma == df$TARGET[INDEX]) | pos == 'ADV')
TARGET_mod <- TARGET_mod$lemma[TARGET_mod$pos == 'ADV' & TARGET_mod$token_id == TARGET_mod$token_id[TARGET_mod$pos == 'ADJ'] - 1]
TARGET_mod <- ifelse(identical(TARGET_mod, character(0)), NA, TARGET_mod)
ADV <- filter(POS, (pos == 'ADJ' & !lemma == df$TARGET[INDEX]) | pos == 'ADV')
ADV <- ADV$lemma[ADV$pos == 'ADV' & ADV$token_id == ADV$token_id[ADV$pos == 'ADJ'] - 1]
ADV <- ifelse(identical(ADV, character(0)), NA, ADV)
first <- filter(POS, pos == 'ADJ')
first <- ifelse(first$token_id[first$lemma == df$TARGET[INDEX]] < first$token_id[!first$lemma == df$TARGET[INDEX]], 1, 0)
dta <- tibble(match = y, ADJ, TARGET_mod, ADV, first)
return(dta)
}
}), warning = function(e) print(INDEX))
if(is.list(tmp)) tmp <- do.call(rbind, tmp)
if(is.list(tmp)|is.null(tmp)) return(tmp)
}
for(i in datasets){
#i=datasets[1]
load(i)
print(i)
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
df <- mutate(df, CCONJ = sapply(TARGET, function(x) return(x[x %in% c('and', 'or')][1])))
df <- mutate(df, TARGET = sapply(TARGET, function(x) return(x[x %in% search.terms$word][1])))
df <- mutate(df, comma = grepl('\\,', match))
txtparsed <- spacy_parse(tolower(df$corpus), pos = TRUE)
txtparsed <- split(txtparsed, txtparsed$doc_id, lex.order = F)
txtparsed <- txtparsed[mixedsort(names(txtparsed))]
txtparsed <- pbmclapply(txtparsed, function(x) x$lemma %>% setNames(., x$pos), mc.cores = 4)
system.time(txtparsed_adj <- pbmclapply(1:length(txtparsed), make_regex, mc.cores=4))
#system.time(txtparsed_adj <- lapply(1:length(txtparsed), make_regex))
#df <- rename(df, comma_check = comma, TARGET_check = TARGET, CCONJ_check = CCONJ)
reps <- unlist(lapply(sapply(txtparsed_adj, nrow), function(x) ifelse(is.null(x), 0, x)))
df <- df[rep(1:nrow(df), reps),]
txtparsed_adj <- do.call(rbind, txtparsed_adj)
#txtparsed_adj <- mutate(txtparsed_adj, id=df$id)
#txtparsed_adj <- select(txtparsed_adj, -id)
df <- rename(df, match_first = match)
df <- cbind(df, txtparsed_adj)
df <- as_tibble(df)
#table(df$TARGET)
df <- filter(df, TARGET%in%search.terms$word)
#table(df$TARGET)
out <- paste0('../../output/02-finalized-corpora/baseline/reddit/new-', gsub('.*\\/', '', i))
save(df, file = out)
}
### generate full corpus
fileslist <- list.files('../../output/02-finalized-corpora/baseline/reddit', full.names = T, pattern = 'new\\-')
reddit <- pbmclapply(fileslist, function(x){
load(x)
return(df)
})
reddit <- do.call(rbind, reddit)
reddit <- as_tibble(reddit)
reddit <- mutate(reddit, context = 'reddit')
save(reddit, file = '../../output/02-finalized-corpora/baseline/reddit/new-reddit.RDS', compress = T)
library(stringr)
library(stringi)
library(spacyr)
library(gtools)
library(tokenizers)
library(dplyr)
library(reticulate)
library(pbmcapply)
rm(list=ls())
setwd('~/legal_tc/src/legal/')
#spacy_uninstall()
#spacy_install()
#use_condaenv(condaenv = 'spacy_condaenv', conda = "auto", required = FALSE)
spacy_initialize()
datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T)
datasets
datasets <- list.files('../../output/01-reduced-corpora/legal', full.names = T, pattern = 'new')
datasets
datasets <- datasets[!grepl('scotus', datasets)]
search.terms <- read.table('../../input/dict-add.txt', header = T, stringsAsFactors = F, sep=',')
syntax.regex <- '(ADV\\s)?ADJ\\s(PUNCT\\s)?CCONJ\\s(ADV\\s)?ADJ'
make_regex <- function(INDEX){
TARGET = paste0('\\\\b',df$TARGET[INDEX], '\\\\b')
LEMMA = paste0(txtparsed[[INDEX]], collapse = ' ')
SYNTAX = paste0(names(txtparsed[[INDEX]]), collapse = ' ')
SYNTAX <- unlist(str_extract(SYNTAX, syntax.regex))
SYNTAX <- unique(SYNTAX)
tmp <- str_replace_all(SYNTAX, c(
'ADV' = '\\\\w+',
'CCONJ' = 'and',
'PUNCT' = '\\\\,'
))
regex1 <- sub('ADJ', TARGET, tmp)
regex1 <- str_replace_all(regex1, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
regex2 <- stri_replace_last(tmp, replacement = TARGET, regex = 'ADJ')
regex2 <- str_replace_all(regex2, c(
' ' = '\\\\s',
'ADJ' = '\\\\w+'
))
match1 <- unlist(str_extract_all(LEMMA, regex1))
match2 <- unlist(str_extract_all(LEMMA, regex2))
MATCH <- unique(unlist(c(match1, match2)))
if(length(MATCH)==0){return(NULL)}else{
tryCatch(tmp <- lapply(MATCH, function(y){
POS <- spacy_parse(y, pos = T)
if(paste0(POS$pos, collapse = ' ') %in% SYNTAX){
ADJ <- filter(POS, pos == 'ADJ' & !lemma == df$TARGET[INDEX])$lemma
TARGET_mod <- filter(POS, (pos == 'ADJ' & lemma == df$TARGET[INDEX]) | pos == 'ADV')
TARGET_mod <- TARGET_mod$lemma[TARGET_mod$pos == 'ADV' & TARGET_mod$token_id == TARGET_mod$token_id[TARGET_mod$pos == 'ADJ'] - 1]
TARGET_mod <- ifelse(identical(TARGET_mod, character(0)), NA, TARGET_mod)
ADV <- filter(POS, (pos == 'ADJ' & !lemma == df$TARGET[INDEX]) | pos == 'ADV')
ADV <- ADV$lemma[ADV$pos == 'ADV' & ADV$token_id == ADV$token_id[ADV$pos == 'ADJ'] - 1]
ADV <- ifelse(identical(ADV, character(0)), NA, ADV)
first <- filter(POS, pos == 'ADJ')
first <- ifelse(first$token_id[first$lemma == df$TARGET[INDEX]] < first$token_id[!first$lemma == df$TARGET[INDEX]], 1, 0)
dta <- tibble(match = y, ADJ, TARGET_mod, ADV, first)
return(dta)
}
}), warning = function(e) print(INDEX), error = function(e) print(INDEX))
tmp <- do.call(rbind, tmp)
return(tmp)
}
}
