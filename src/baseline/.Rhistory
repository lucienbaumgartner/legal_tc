tokens_lookup('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même',
dictionary = dict, valuetype = "glob")
tokens_lookup(tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même'),
dictionary = dict, valuetype = "glob")
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
test
str(dict)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict
dict <- split(dict, dict$sentiment)
dict <- lapply(dict, function(x) select(x, -sentiment))
dict
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict
dict <- dictionary(dict)
dict <- split(dict, dict$sentiment)
dict <- lapply(dict, function(x) select(x, -sentiment))
dict <- dictionary(dict)
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
str(dict)
l <- tokens_lookup(test,
dictionary = dict)
list(positive = c('Révolution', 'comme', 'apéro', 'grève', 'monsieur', 'omelette'),
negative = c('Dire', 'et'))
p <- list(positive = c('Révolution', 'comme', 'apéro', 'grève', 'monsieur', 'omelette'),
negative = c('Dire', 'et'))
dict2 <- dictionary(p)
dict2
l <- tokens_lookup(test,
dictionary = dict2)
l
p
dict2 <- dictionary(p)
dict2
l <- tokens_lookup(test,
dictionary = dict2)
l
str(FEEL_fr)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
sum(lengths(dict))
sum(sapply(dict, nrow))
dict <- lapply(dict, function(x) select(x, -sentiment))
sum(sapply(dict, nrow))
head(dict$negative, 500)
head(dict$negative, 500) %>%  as.vector()
head(dict$negative, 500) %>% unlist
library(pbmcapply)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
set.seed(123)
dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict
dict <- lapply(dict, function(x) select(x, -sentiment))
dict <- dictionary(dict)
system.time(
l <- tokens_lookup(test,
dictionary = dict)
)
system.time(
l <- pbmclapply(unlist(test), function(x) tokens_lookup(tokens(x),
dictionary = dict), mc.cores =5)
)
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+', x$word), ])
dict
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
dict
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+', x$word), ])
dict
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
dict$negative
dict <- lapply(dict, function(x) select(x, -sentiment))
head(dict$negative, 500) %>% unlist
dict <- as_tibble(FEEL_fr) %>% mutate(sentiment = ifelse(y==-1, 'negative', 'positive')) %>% rename(word = x) %>% select(-y)
dict <- split(dict, dict$sentiment)
#dict <- lapply(dict, function(x) x[sample(1:nrow(x), 200), ])
dict <- lapply(dict, function(x) x[!grepl('\\s+|\\|', x$word), ])
dict <- lapply(dict, function(x) select(x, -sentiment))
head(dict$negative, 500) %>% unlist
sum(sapply(dict, nrow))
library(udpipe)
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
str(FEEL_fr)
dict <- as_tibble(FEEL_fr) %>% rename(polarity = y, term = x)
dict
l <- txt_sentiment(test, polarity_terms = dict)
test <- tokens('Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même')
udpipe(test, "french-spoken", trace = 10)
test <- 'Révolution comme apéro grève monsieur omelette. Dire et paf le chien carrément putain merde frenchtech. Baguette boulangerie voir putain notre évidemment. Part révolution épicé car dans comme même'
udpipe(test, "french-spoken", trace = 10)
test <- udpipe(test, "french-spoken", trace = 10)
l <- txt_sentiment(test, polarity_terms = dict)
l
l
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/FEEL_fr.rda"))
load(file("https://github.com/sborms/sentometrics/raw/master/data-raw/valence-raw/valShifters.rda"))
polarity_terms <- rename(FEEL_fr, term = x, polarity = y)
polarity_negators <- subset(valShifters$valence_fr, t == 1)$x
polarity_amplifiers <- subset(valShifters$valence_fr, t == 2)$x
polarity_deamplifiers <- subset(valShifters$valence_fr, t == 3)$x
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(test, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments
polarity_negators
polarity_amplifiers
load("~/Downloads/NR_SR_Jan_Okt_2019.RData")
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
txt <- romandie_tweets2$Text
head(txt)
txt <- txt[sample(1:length(txt), 200)]
txt
romandie_tweets2 <- udpipe(txt, "french-spoken", trace = 10)
romandie_tweets2
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(txt, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
##
## Do sentiment analysis based on that open French lexicon
##
sentiments <- txt_sentiment(romandie_tweets2, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments
# Prepping the dict
romandie_udpipe <- udpipe(txt, "french-spoken", trace = 10)
sentiments
# graph
library(magrittr)
library(ggraph)
library(igraph)
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
sentiments
# sentiment
sentiments_txt <- txt_sentiment(romandie_udpipe, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments <- sentiments_txt$data
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
reasons <- filter(reasons, dep_rel %in% "amod")
word_cooccurences <- reasons %>%
group_by(lemma, lemma_parent) %>%
summarise(cooc = n()) %>%
arrange(-cooc)
vertices <- bind_rows(
data_frame(key = unique(reasons$lemma)) %>% mutate(in_dictionary = if_else(key %in% polarity_terms$term, "in_dictionary", "linked-to")),
data_frame(key = unique(setdiff(reasons$lemma_parent, reasons$lemma))) %>% mutate(in_dictionary = "linked-to"))
# graph
library(magrittr)
library(ggraph)
library(igraph)
cooc <- head(word_cooccurences, 20)
set.seed(123)
cooc %>%
graph_from_data_frame(vertices = filter(vertices, key %in% c(cooc$lemma, cooc$lemma_parent))) %>%
ggraph(layout = "fr") +
geom_edge_link0(aes(edge_alpha = cooc, edge_width = cooc)) +
geom_node_point(aes(colour = in_dictionary), size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
ggtitle("Which words are linked to the negative terms") +
theme_void()
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 200)]
head(romandie_txt)
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_tweets2$Text
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_tweets2
romandie_tweets2$Text
head(romandie_tweets2)
romandie_tweets2 <- subset(NR_SR_Jan_Okt_2019, Lang == "fr")
romandie_tweets2
romandie_tweets2
romandie_tweets2$Text
romandie_txt <- romandie_tweets2$Text
head(romandie_txt)
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 200)]
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_txt
romandie_txt <- romandie_tweets2$Text
romandie_txt <- romandie_txt[sample(1:length(romandie_txt), 500)]
romandie_udpipe <- udpipe(romandie_txt, "french-spoken", trace = 10)  # only using a small sample first to figure out all the settings and not waste time
# sentiment
sentiments_txt <- txt_sentiment(romandie_udpipe, term = "lemma",
polarity_terms = polarity_terms,
polarity_negators = polarity_negators,
polarity_amplifiers = polarity_amplifiers,
polarity_deamplifiers = polarity_deamplifiers)
sentiments <- sentiments_txt$data
reasons <- sentiments %>%
cbind_dependencies() %>%
select(doc_id, lemma, token, upos, sentiment_polarity, token_parent, lemma_parent, upos_parent, dep_rel) %>%
filter(sentiment_polarity < 0)
reasons <- filter(reasons, dep_rel %in% "amod")
word_cooccurences <- reasons %>%
group_by(lemma, lemma_parent) %>%
summarise(cooc = n()) %>%
arrange(-cooc)
vertices <- bind_rows(
data_frame(key = unique(reasons$lemma)) %>% mutate(in_dictionary = if_else(key %in% polarity_terms$term, "in_dictionary", "linked-to")),
data_frame(key = unique(setdiff(reasons$lemma_parent, reasons$lemma))) %>% mutate(in_dictionary = "linked-to"))
cooc <- head(word_cooccurences, 20)
set.seed(123)
cooc %>%
graph_from_data_frame(vertices = filter(vertices, key %in% c(cooc$lemma, cooc$lemma_parent))) %>%
ggraph(layout = "fr") +
geom_edge_link0(aes(edge_alpha = cooc, edge_width = cooc)) +
geom_node_point(aes(colour = in_dictionary), size = 5) +
geom_node_text(aes(label = name), vjust = 1.8, col = "darkgreen") +
ggtitle("Which words are linked to the negative terms") +
theme_void()
install.packages('qualtRics')
library(qualtRics)
library(dplyr)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.align = 'center')
filepath <- '/Users/lucienbaumgartner/Dropbox/thesis/embed.R/input/TC+Embeddings+Valence+Control_April+28,+2020_09.13.csv'
library(dplyr)
library(pbmcapply)
library(httr)
library(lubridate)
library(anytime)
library(stringi)
library(rlist)
library(Hmisc)
rm(list=ls())
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
file_paths <- list.files('../output/00-bulk-data/baseline/reddit/raw/', full.names = T)
df <- pbmclapply(file_paths, function(x){
load(x)
return(df$body)
}, mc.cores = 4)
df <- unlist(df)
df <- as_tibble(df)
file_paths <- list.files('../output/00-bulk-data/baseline/reddit/raw/', full.names = T)
df <- pbmclapply(file_paths, function(x){
load(x)
return(df$body)
}, mc.cores = 4)
df <- unlist(df)
file_paths <- list.files('../output/00-bulk-data/baseline/reddit/raw/', full.names = T)
df <- pbmclapply(file_paths, function(x){
load(x)
return(df$body)
}, mc.cores = 4)
names(df) <- gsub('\\_.*','', list.files('../output/00-bulk-data/baseline/reddit/raw/')
names(df) <- gsub('\\_.*','', list.files('../output/00-bulk-data/baseline/reddit/raw/'))
names(df) <- gsub('\\_.*','', list.files('../output/00-bulk-data/baseline/reddit/raw/'))
df
for(i in unique(names(df))){
print(i)
dta <- unlist(df[grepl(i, names(df))])
if(!is.null(dta)){
names(dta) <- i
save(dta, file = paste0('../output/00-bulk-data/baseline/reddit/raw/', i, '.RDS'))
}
}
for(i in unique(names(df))){
print(i)
dta <- unlist(df[grepl(i, names(df))])
if(!is.null(dta)){
names(dta) <- i
save(dta, file = paste0('../output/00-bulk-data/baseline/reddit/raw_aggr/', i, '.RDS'))
}
}
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
rm(list=ls())
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
datasets <- list.files('../output/00-bulk-data/reddit/raw_aggr', full.names = T)
datasets <- list.files('../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T)
datasets
search.terms <- read.table('../input/dict.txt', header = T, stringsAsFactors = F, sep=',')
i = datasets[1]
#i = datasets[1]
load(i)
.lookup <- paste0(paste0('\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b', collapse = '|')
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
corpus <- pbmclapply(dta, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
df <- tibble(txt=dta)
.lookup <- paste0(paste0('\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b', collapse = '|')
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
df
i
# save data
out <- paste0('../output/01-reduced-corpora/baseline/', gsub('.*\\/', '', i))
out
save(df, file = out)
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
rm(list=ls())
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
datasets <- list.files('../../output/00-bulk-data/baseline/reddit/raw_aggr', full.names = T)
search.terms <- read.table('../../input/dict.txt', header = T, stringsAsFactors = F, sep=',')
for(i in datasets){
#i = datasets[1]
load(i)
df <- tibble(txt=dta)
.lookup <- paste0(paste0('\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b', collapse = '|')
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
# save data
out <- paste0('../../output/01-reduced-corpora/baseline/', gsub('.*\\/', '', i))
save(df, file = out)
}
for(i in datasets){
#i = datasets[1]
load(i)
df <- tibble(txt=dta)
.lookup <- paste0(paste0('\\b', search.terms$word, '\\b'), '(\\,)?\\s\\band\\b', collapse = '|')
corpus <- pbmclapply(df$txt, function(x){
#print(x)
tmp <- tokenizers::tokenize_sentences(x)
tmp <- unlist(tmp)
tmp <- tolower(tmp)
tmp <- tmp[grepl(.lookup, tolower(tmp), perl = T)]
tmp <- unname(tmp)
return(tmp)
}, mc.cores = 4)
df <- cbind(df[rep(1:nrow(df), lengths(corpus)),], corpus=unlist(corpus)) %>% as_tibble
rm(corpus)
df <- mutate(df, corpus = as.character(corpus))
reg_matches <- pbmclapply(df$corpus, function(x) str_extract_all(x, .lookup), mc.cores=4)
reg_matches <- unlist(reg_matches, recursive=F)
df <- cbind(df[rep(1:nrow(df), lengths(reg_matches)),], match=unlist(reg_matches)) %>% as_tibble
df <- mutate(df, match = as.character(match))
df <- as_tibble(df)
# save data
out <- paste0('../../output/01-reduced-corpora/baseline/reddit/', gsub('.*\\/', '', i))
save(df, file = out)
}
library(stringr)
library(spacyr)
library(gtools)
library(tokenizers)
rm(list=ls())
getwd()
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
datasets <- list.files('../output/01-reduced-corpora/baseline/reddit', full.names = T)
datasets <- list.files('../output/01-reduced-corpora/baseline/reddit/', full.names = T)
datasets <- list.files('../../output/01-reduced-corpora/baseline/reddit/', full.names = T)
search.terms <- read.table('../../input/dict.txt', header = T, stringsAsFactors = F, sep=',')
i=datasets[1]
#i=datasets[1]
load(i)
df <- mutate(df, TARGET = strsplit(gsub('\\,', '', match), '\\s'))
df <- mutate(df, CCONJ = sapply(TARGET, '[[', 2))
df <- mutate(df, TARGET = sapply(TARGET, '[[', 1))
df <- mutate(df, comma = grepl('\\,', match))
txtparsed <- spacy_parse(tolower(df$corpus), pos = TRUE)
txtparsed <- split(txtparsed, txtparsed$doc_id, lex.order = F)
txtparsed <- txtparsed[mixedsort(names(txtparsed))]
txtparsed <- pbmclapply(txtparsed, function(x) x$lemma %>% setNames(., x$pos), mc.cores = 4)
#.lookup <- strsplit(gsub('\\\\b|\\(\\,\\)\\?', '', .lookup), '\\s') %>% unlist
txtparsed_adj <- pbmclapply(1:length(txtparsed), function(INDEX){
z <- txtparsed[[INDEX]]
.lookup <- c(df$TARGET[INDEX], df$CCONJ[INDEX])
windex <- grep(.lookup[1], z) # lookup the ADJ
# if the word afterwards is the corresponding AND/BUT OR
# comma followed by the corresponding AND/BUT
windex <- windex[!is.na(z[windex + 2])]
if(!any(is.na(windex)|is.null(windex))){
if(any( z[windex + 1] == .lookup[2] | (z[windex + 1] == ',' & z[windex + 2] == .lookup[2]) )){
tmp <- lapply(windex, function(y, pop=z){
p <- ifelse(pop[y + 1] == ',' & pop[y + 2] == .lookup[2], 1, 0)
if(y+ 2 +p <length(pop)){
if(pop[y + 1 + p] == .lookup[2] & ( names(pop[y + 2 + p]) == 'ADJ' | ( names(pop[y + 2 + p]) == 'ADV' & ifelse('try-error'%in%class(try(names(pop[y + 3 + p]) == 'ADJ')), F, names(pop[y + 3 + p]) == 'ADJ') ) ) ){
if(y-1>0){
pop <- pop[(y-1):length(pop)]
}else{
pop <- c(NA, pop[y:length(pop)])
}
pop <- tryCatch({pop[1:grep('ADJ', names(pop))[2]]}, error = function(e) NULL)
if(!is.null(pop)){
pop <- tibble(TARGET = pop[2], CCONJ = pop[3 + p], ADV = ifelse(names(pop[4 + p]) == 'ADV', pop[4 + p], NA),
ADJ = ifelse(is.na(ADV), pop[4 + p], pop[5 + p]), modifier = ifelse(names(pop[1]) == 'ADV', pop[1], NA),
comma = p)
}
return(pop)
}
}
})
tmp <- do.call(rbind, tmp)
return(tmp)
}
}
}
, mc.cores=4)
df <- rename(df, comma_check = comma, TARGET_check = TARGET, CCONJ_check = CCONJ)
reps <- unlist(lapply(sapply(txtparsed_adj, nrow), function(x) ifelse(is.null(x), 0, x)))
df <- df[rep(1:nrow(df), reps),]
txtparsed_adj <- do.call(rbind, txtparsed_adj)
#txtparsed_adj <- mutate(txtparsed_adj, id=df$id)
#txtparsed_adj <- select(txtparsed_adj, -id)
df <- cbind(df, txtparsed_adj)
df <- as_tibble(df)
df <- filter(df, TARGET%in%search.terms$word)
df
head(df$corpus)
#table(df$TARGET)
out <- paste0('../output/02-finalized-corpora/baseline/reddit/', gsub('.*\\/', '', i))
out
save(df, file = out)
#table(df$TARGET)
out <- paste0('../../output/02-finalized-corpora/baseline/reddit/', gsub('.*\\/', '', i))
save(df, file = out)
#table(df$TARGET)
out <- paste0('../../output/02-finalized-corpora/baseline/reddit/', gsub('.*\\/', '', i))
save(df, file = out)
load('/Users/lucienbaumgartner/legal_tc/output/02-finalized-corpora/legal/scotus.RDS')
table(df$TARGET)
tavle(df$ADJ)
table(df$ADJ)
dfx <- table(df$ADJ)
dfx <- dfx[order(dfx)]
dfx
