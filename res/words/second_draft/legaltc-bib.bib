@inproceedings{Widdows2008,
abstract = {This paper describes the open source SemanticVectors package that efficiently creates semantic vectors for words and documents from a corpus of free text articles. We believe that this package can play an important role in furthering research in distributional semantics, and (perhaps more importantly) can help to significantly reduce the current gap that exists between good research results and valuable applications in production software. Two clear principles that have guided the creation of the package so far include ease-of-use and scalability. The basic package installs and runs easily on any Java-enabled platform, and depends only on Apache Lucene. Dimension reduction is performed using Random Projection, which enables the system to scale much more effectively than other algorithms used for the same purpose. This paper also describes a trial application in the Technology Management domain, which highlights some user-centred design challenges that we believe are also key to successful deployment of this technology.},
author = {Widdows, Dominic and Ferraro, Kathleen},
booktitle = {In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2008)},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Widdows, Ferraro - Unknown - Semantic Vectors A Scalable Open Source Package and Online Technology Management Application.pdf:pdf},
title = {{Semantic Vectors: A Scalable Open Source Package and Online Technology Management Application}},
url = {http://semanticvectors.googlecode.com.},
year = {2008}
}
@inproceedings{Widdows2010,
abstract = {Distributional semantics is the branch of natural language processing that attempts to model the meanings of words, phrases and documents from the distribution and usage of words in a corpus of text. In the past three years, research in this area has been accelerated by the availability of the Semantic Vectors package, a stable, fast, scalable, and free software package for creating and exploring concepts in distributional models. This paper introduces the broad field of distributional semantics, the role of vector models within this field, and describes some of the results that have been made possible by the Semantic Vectors package. These applications of Semantic Vectors have so far included contributions to medical informatics and knowledge discovery, analysis of scientific articles, and even Biblical scholarship. Of particular interest is the recent emergence of models that take word order and other ordered structures into account, using permutation of coordinates to model directional relationships and semantic predicates. {\textcopyright} 2010 IEEE.},
author = {Widdows, Dominic and Cohen, Trevor},
booktitle = {Proceedings - 2010 IEEE 4th International Conference on Semantic Computing, ICSC 2010},
doi = {10.1109/ICSC.2010.94},
isbn = {9780769541549},
pages = {9--15},
title = {{The semantic vectors package: New algorithms and public tools for distributional semantics}},
year = {2010}
}
@inproceedings{Widdows2016,
abstract = {Semantic vector models are traditionally used to model concepts derived from discrete input such as tokenized text. This paper describes a technique to address continuous and graded quantities using such models. The method presented here grows out of earlier work on modelling orthography, or letter-by-letter word encoding, in which a graded vector is used to model character-positions within a word. We extend this idea to use a graded vector for a position along any scale. The technique is applied to modelling time-periods in an example dataset of Presidents of the United States. Initial examples demonstrate that encoding the time-periods using graded semantic vectors gives an improvement over modelling the dates in question as distinct strings. This work is significant because it fills a surprising technical gap: Though vector spaces over a continuous ground-field seem a natural choice for representing graded quantities, this capability has been hitherto lacking, and is a necessary step towards a more complete vector space model of conceptualization and cognition.},
author = {Widdows, Dominic and Cohen, Trevor},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-28675-4_18},
isbn = {9783319286747},
issn = {16113349},
pages = {231--244},
publisher = {Springer Verlag},
title = {{Graded semantic vectors: An approach to representing graded quantities in generalized quantum models}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-28675-4{\_}18},
volume = {9535},
year = {2016}
}
@book{Yule1944,
author = {Yule, George Udny},
publisher = {Cambridge University Press},
title = {{The Statistical Study of Literary Vocabulary}},
year = {1944}
}
@techreport{Tweedie1998,
abstract = {A well-known problem in the domain of quantitative linguistics and stylistics concerns the evaluation of the lexical richness of texts. Since the most obvious measure of lexical richness, the vocabulary size (the number of different word types), depends heavily on the text length (measured in word tokens), a variety of alternative measures has been proposed which are claimed to be independent of the text length. This paper has a threefold aim. Firstly, we have investigated to what extent these alternative measures are truly textual constants. We have observed that in practice all measures vary substantially and systematically with the text length. We also show that in theory, only three of these measures are truly constant or nearly constant. Secondly, we have studied the extent to which these measures tap into different aspects of lexical structure. We have found that there are two main families of constants, one measuring lexical richness and one measuring lexical repetition. Thirdly, we have considered to what extent these measures can be used to investigate questions of textual similarity between and within authors. We propose to carry out such comparisons by means of the empirical trajectories of texts in the plane spanned by the dimensions of lexical richness and lexical repetition, and we provide a statistical technique for constructing confidence intervals around the empirical trajectories of texts. Our results suggest that the trajectories tap into a considerable amount of authorial structure without, however, guaranteeing that spatial separation implies a difference in authorship.},
author = {Tweedie, Fiona J and Baayen, R Harald},
booktitle = {Computers and the Humanities},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Tweedie, Baayen - 1998 - How Variable May a Constant be Measures of Lexical Richness in Perspective.pdf:pdf},
keywords = {Monte Carlo methods,lexical statistics,vocabulary richness},
pages = {323--352},
title = {{How Variable May a Constant be? Measures of Lexical Richness in Perspective}},
volume = {32},
year = {1998}
}
@inproceedings{Straka2020,
abstract = {We present our contribution to the EvaLatin shared task, which is the first evaluation campaign devoted to the evaluation of NLP tools for Latin. We submitted a system based on UDPipe 2.0, one of the winners of the CoNLL 2018 Shared Task, The 2018 Shared Task on Extrinsic Parser Evaluation and SIGMORPHON 2019 Shared Task. Our system places first by a wide margin both in lemmatization and POS tagging in the open modality, where additional supervised data is allowed, in which case we utilize all Universal Dependency Latin treebanks. In the closed modality, where only the EvaLatin training data is allowed, our system achieves the best performance in lemmatization and in classical subtask of POS tagging, while reaching second place in cross-genre and cross-time settings. In the ablation experiments, we also evaluate the influence of BERT and XLM-RoBERTa contextualized embeddings, and the treebank encodings of the different flavors of Latin treebanks.},
archivePrefix = {arXiv},
arxivId = {2006.03687},
author = {Straka, Milan and Strakov{\'{a}}, Jana},
booktitle = {Proceedings of Language Resources and Evaluation},
eprint = {2006.03687},
keywords = {BERT,EvaLatin,Lemmatization,POS tagging,UDPipe,XLM-RoBERTa},
month = {jun},
publisher = {arXiv},
title = {{UDPipe at EvaLatin 2020: Contextualized Embeddings and Treebank Embeddings}},
year = {2020}
}
@inproceedings{Straka2017,
abstract = {We present an update to UDPipe 1.0 (Straka et al., 2016), a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing. We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format.},
author = {Straka, Milan and Strakov{\'{a}}, Jana},
booktitle = {Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Straka, Strakov{\'{a}} - Unknown - Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe.pdf:pdf},
title = {{Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe}},
url = {http://ufal.mff.cuni.cz/udpipe},
year = {2017}
}
@techreport{Baumgartner2020,
abstract = {Social media data has become crucial to the advancement of scientific understanding. However, even though it has become ubiquitous, just collecting large-scale social media data involves a high degree of engineering skill set and computational resources. In fact, research is often times gated by data engineering problems that must be overcome before analysis can proceed. This has resulted recognition of datasets as meaningful research contributions in and of themselves. Reddit, the so called "front page of the Internet," in particular has been the subject of numerous scientific studies. Although Reddit is relatively open to data acquisition compared to social media platforms like Facebook and Twitter, the technical barriers to acquisition still remain. Thus, Reddit's millions of subreddits, hundreds of millions of users, and hundreds of billions of comments are at the same time relatively accessible , but time consuming to collect and analyze systematically. In this paper, we present the Pushshift Reddit dataset. Pushshift is a social media data collection, analysis, and archiving platform that since 2015 has collected Reddit data and made it available to researchers. Pushshift's Reddit dataset is updated in real-time, and includes historical data back to Reddit's inception. In addition to monthly dumps, Pushshift provides computational tools to aid in searching, aggregat-ing, and performing exploratory analysis on the entirety of the dataset. The Pushshift Reddit dataset makes it possible for social media researchers to reduce time spent in the data collection , cleaning, and storage phases of their projects.},
archivePrefix = {arXiv},
arxivId = {2001.08435v1},
author = {Baumgartner, Jason and Zannettou, Savvas and Keegan, Brian and Squire, Megan and Blackburn, Jeremy and Io, Pushshift},
eprint = {2001.08435v1},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Baumgartner et al. - Unknown - The Pushshift Reddit Dataset.pdf:pdf},
pages = {1--11},
title = {{The Pushshift Reddit Dataset}},
url = {https://files.pushshift.io/reddit/},
year = {2020}
}
@misc{FreeLawProject2020,
author = {{Free Law Project}},
booktitle = {Court Listener API},
title = {{Bulk Data â€“ CourtListener.com}},
url = {https://www.courtlistener.com/api/bulk-info/},
urldate = {2020-07-15},
year = {2020}
}
@inproceedings{Baccianella2010,
abstract = {In this work we present SENTIWORDNET 3.0, a lexical resource explicitly devised for supporting sentiment classification and opinion mining applications. SENTIWORDNET 3.0 is an improved version of SENTIWORDNET 1.0, a lexical resource publicly available for research purposes, now currently licensed to more than 300 research groups and used in a variety of research projects worldwide. Both SENTIWORDNET 1.0 and 3.0 are the result of automatically annotating all WORDNET synsets according to their degrees of positivity, negativity, and neutrality. SENTIWORDNET 1.0 and 3.0 differ (a) in the versions of WORDNET which they annotate (WORDNET 2.0 and 3.0, respectively), (b) in the algorithm used for automatically annotating WORDNET, which now includes (additionally to the previous semi-supervised learning step) a random-walk step for refining the scores. We here discuss SENTIWORDNET 3.0, especially focussing on the improvements concerning aspect (b) that it embodies with respect to version 1.0. We also report the results of evaluating SENTIWORDNET 3.0 against a fragment of WORDNET 3.0 manually annotated for positivity, negativity, and neutrality; these results indicate accuracy improvements of about 20{\%} with respect to SENTIWORDNET 1.0.},
author = {Baccianella, Stefano and Esuli, Andrea and Sebastiani, Fabrizio},
booktitle = {Proceedings of the 7th International Conference on Language Resources and Evaluation},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Baccianella, Esuli, Sebastiani - Unknown - SENTIWORDNET 3.0 An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining.pdf:pdf},
pages = {2200--2204},
title = {{SENTIWORDNET 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining}},
url = {http://wordnetcode.princeton.},
year = {2010}
}
@inproceedings{Esuli2006,
abstract = {Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users' opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the "PN-polarity" of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), P os(s) and N eg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.},
author = {Esuli, Andrea and Sebastiani, Fabrizio},
booktitle = {Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Esuli, Sebastiani - Unknown - SENTIWORDNET A Publicly Available Lexical Resource for Opinion Mining.pdf:pdf},
pages = {417--422},
title = {{SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining}},
url = {http://www-2.cs.cmu.edu/},
year = {2006}
}
@article{Guerini2013,
abstract = {Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis. In the literature, various approaches based on SentiWordNet have been proposed. In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores. Using two different versions of SentiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-of-the-art approach in computing words' prior polarity for sentiment analysis. We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.},
archivePrefix = {arXiv},
arxivId = {1309.5843},
author = {Guerini, Marco and Gatti, Lorenzo and Turchi, Marco},
eprint = {1309.5843},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Guerini, Gatti, Turchi - 2013 - Sentiment Analysis How to Derive Prior Polarities from SentiWordNet.pdf:pdf},
journal = {EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference},
month = {sep},
pages = {1259--1269},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Sentiment Analysis: How to Derive Prior Polarities from SentiWordNet}},
url = {http://arxiv.org/abs/1309.5843},
year = {2013}
}
@article{Gatti2016,
abstract = {Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called 'ensemble method'). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica.},
author = {Gatti, Lorenzo and Guerini, Marco and Turchi, Marco},
doi = {10.1109/TAFFC.2015.2476456},
file = {:Users/lucienbaumgartner/Library/Application Support/Mendeley Desktop/Downloaded/Gatti, Guerini, Turchi - 2016 - SentiWords Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis.pdf:pdf},
issn = {19493045},
journal = {IEEE Transactions on Affective Computing},
keywords = {Natural language processing,machine learning,text analysis},
month = {oct},
number = {4},
pages = {409--421},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis}},
volume = {7},
year = {2016}
}
